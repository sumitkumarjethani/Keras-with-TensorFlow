{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bdd3d07",
   "metadata": {},
   "source": [
    "In this episode, we'll demonstrate how to process numerical data that we'll later use to train our very first artificial neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d813128",
   "metadata": {},
   "source": [
    "## Samples and Labels\n",
    "\n",
    "To train any neural network in a supervised learning task, we first need a data set of samples and the corresponding labels for those samples.\n",
    "\n",
    "When referring to samples, we're just referring to the underlying data set, where each individual item or data point within that set is called a sample. Labels are the corresponding labels for the samples.\n",
    "\n",
    "**Note that in deep learning, samples are also commonly referred to as input data or inputs, and labels are also commonly referred to as target data or targets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7ce9e",
   "metadata": {},
   "source": [
    "###  Expected data format\n",
    "\n",
    "When preparing data, we first need to understand the format that the data need to be in for the end goal we have in mind. In our case, we want our data to be in a format that we can pass to a neural network model.\n",
    "\n",
    "The first model we'll build in an upcoming episode will be a **Sequential model** from the Keras API integrated within TensorFlow.\n",
    "\n",
    "The Sequential model receives data during training, which occurs when we call the ***fit()*** function on the model.\n",
    "\n",
    "[Documentation of fit() function](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit)\n",
    "\n",
    "In the ***fit()*** function: **x** is the input data and **y** are the labels for that input data in the same format or data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab697b1a",
   "metadata": {},
   "source": [
    "## Process data in code\n",
    "\n",
    "We'll start out with a very simple classification task using a simple numerical data set.\n",
    "\n",
    "We first need to import the libraries we'll be working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4057065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e02e5",
   "metadata": {},
   "source": [
    "Next, we create two empty lists. One will hold the **input data**, the other will hold the **target data or labels**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4ecf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e13e4",
   "metadata": {},
   "source": [
    "### Data Creation\n",
    "\n",
    "For this simple task, we'll be creating our own example data set.\n",
    "\n",
    "As motivation for this data, let's suppose that an experimental drug was tested on individuals ranging from age 13 to 100 in a clinical trial. The trial had **2100** participants. Half of the participants were under 65 years old, and the other half was 65 years of age or older.\n",
    "\n",
    "The trial showed that around 95% of patients 65 or older experienced side effects from the drug, and around 95% of patients under 65 experienced no side effects, generally showing that elderly individuals were more likely to experience side effects.\n",
    "\n",
    "Ultimately, we want to build a model to tell us whether or not a patient will experience side effects solely based on the patient's age. The judgement of the model will be based on the training data.\n",
    "\n",
    "**Labels:**\n",
    "- 1: patient did experience side effects\n",
    "- 0: patient didnÂ´t experience side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22aef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    # The ~5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "\n",
    "    # The ~5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    # The ~95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "\n",
    "    # The ~95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cd795",
   "metadata": {},
   "source": [
    "This is what the train_samples data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d588ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "97\n",
      "49\n",
      "85\n",
      "48\n",
      "85\n",
      "57\n",
      "90\n",
      "34\n",
      "69\n",
      "36\n",
      "96\n",
      "45\n",
      "77\n",
      "24\n",
      "96\n",
      "15\n",
      "71\n",
      "15\n",
      "83\n",
      "53\n",
      "69\n",
      "40\n",
      "96\n",
      "30\n",
      "72\n",
      "27\n",
      "99\n",
      "13\n",
      "97\n",
      "18\n",
      "89\n",
      "36\n",
      "66\n",
      "36\n",
      "76\n",
      "60\n",
      "65\n",
      "20\n",
      "73\n",
      "32\n",
      "78\n",
      "33\n",
      "83\n",
      "22\n",
      "78\n",
      "41\n",
      "94\n",
      "21\n",
      "86\n",
      "39\n",
      "98\n",
      "37\n",
      "93\n",
      "58\n",
      "93\n",
      "63\n",
      "77\n",
      "25\n",
      "88\n",
      "39\n",
      "66\n",
      "58\n",
      "79\n",
      "24\n",
      "92\n",
      "36\n",
      "69\n",
      "36\n",
      "69\n",
      "18\n",
      "69\n",
      "20\n",
      "71\n",
      "44\n",
      "89\n",
      "55\n",
      "98\n",
      "55\n",
      "89\n",
      "49\n",
      "84\n",
      "13\n",
      "70\n",
      "56\n",
      "79\n",
      "47\n",
      "98\n",
      "58\n",
      "71\n",
      "55\n",
      "100\n",
      "55\n",
      "89\n",
      "37\n",
      "97\n",
      "25\n",
      "75\n",
      "42\n",
      "99\n",
      "58\n",
      "83\n",
      "51\n",
      "71\n",
      "64\n",
      "79\n",
      "64\n",
      "84\n",
      "29\n",
      "66\n",
      "64\n",
      "83\n",
      "60\n",
      "83\n",
      "24\n",
      "68\n",
      "53\n",
      "90\n",
      "54\n",
      "87\n",
      "26\n",
      "97\n",
      "55\n",
      "89\n",
      "34\n",
      "98\n",
      "15\n",
      "79\n",
      "30\n",
      "87\n",
      "27\n",
      "92\n",
      "28\n",
      "84\n",
      "30\n",
      "69\n",
      "56\n",
      "69\n",
      "52\n",
      "77\n",
      "22\n",
      "79\n",
      "13\n",
      "90\n",
      "13\n",
      "97\n",
      "47\n",
      "92\n",
      "61\n",
      "94\n",
      "64\n",
      "66\n",
      "59\n",
      "89\n",
      "32\n",
      "70\n",
      "55\n",
      "91\n",
      "59\n",
      "65\n",
      "64\n",
      "68\n",
      "23\n",
      "94\n",
      "43\n",
      "68\n",
      "25\n",
      "75\n",
      "32\n",
      "94\n",
      "62\n",
      "99\n",
      "51\n",
      "82\n",
      "35\n",
      "85\n",
      "48\n",
      "65\n",
      "14\n",
      "91\n",
      "19\n",
      "99\n",
      "42\n",
      "67\n",
      "60\n",
      "93\n",
      "60\n",
      "92\n",
      "17\n",
      "80\n",
      "29\n",
      "84\n",
      "55\n",
      "91\n",
      "57\n",
      "74\n",
      "39\n",
      "67\n",
      "18\n",
      "98\n",
      "45\n",
      "94\n",
      "62\n",
      "81\n",
      "58\n",
      "76\n",
      "47\n",
      "76\n",
      "34\n",
      "88\n",
      "55\n",
      "98\n",
      "14\n",
      "88\n",
      "15\n",
      "87\n",
      "54\n",
      "91\n",
      "29\n",
      "65\n",
      "25\n",
      "79\n",
      "33\n",
      "100\n",
      "63\n",
      "97\n",
      "63\n",
      "71\n",
      "20\n",
      "77\n",
      "44\n",
      "91\n",
      "27\n",
      "66\n",
      "24\n",
      "94\n",
      "51\n",
      "96\n",
      "22\n",
      "76\n",
      "26\n",
      "98\n",
      "31\n",
      "79\n",
      "22\n",
      "100\n",
      "21\n",
      "77\n",
      "21\n",
      "93\n",
      "63\n",
      "92\n",
      "42\n",
      "93\n",
      "31\n",
      "76\n",
      "49\n",
      "83\n",
      "20\n",
      "90\n",
      "14\n",
      "97\n",
      "34\n",
      "73\n",
      "29\n",
      "86\n",
      "44\n",
      "99\n",
      "63\n",
      "73\n",
      "64\n",
      "71\n",
      "46\n",
      "72\n",
      "47\n",
      "77\n",
      "54\n",
      "92\n",
      "45\n",
      "83\n",
      "15\n",
      "71\n",
      "61\n",
      "77\n",
      "43\n",
      "86\n",
      "53\n",
      "87\n",
      "16\n",
      "74\n",
      "17\n",
      "65\n",
      "35\n",
      "70\n",
      "50\n",
      "98\n",
      "32\n",
      "86\n",
      "49\n",
      "73\n",
      "25\n",
      "97\n",
      "28\n",
      "84\n",
      "59\n",
      "99\n",
      "58\n",
      "76\n",
      "44\n",
      "83\n",
      "30\n",
      "65\n",
      "49\n",
      "91\n",
      "35\n",
      "97\n",
      "55\n",
      "76\n",
      "47\n",
      "100\n",
      "38\n",
      "70\n",
      "41\n",
      "73\n",
      "36\n",
      "81\n",
      "56\n",
      "70\n",
      "29\n",
      "85\n",
      "22\n",
      "66\n",
      "42\n",
      "100\n",
      "54\n",
      "90\n",
      "26\n",
      "73\n",
      "64\n",
      "93\n",
      "21\n",
      "67\n",
      "33\n",
      "91\n",
      "36\n",
      "96\n",
      "45\n",
      "90\n",
      "45\n",
      "90\n",
      "39\n",
      "97\n",
      "17\n",
      "87\n",
      "28\n",
      "69\n",
      "56\n",
      "74\n",
      "21\n",
      "81\n",
      "43\n",
      "80\n",
      "27\n",
      "68\n",
      "33\n",
      "83\n",
      "34\n",
      "70\n",
      "39\n",
      "100\n",
      "43\n",
      "96\n",
      "26\n",
      "100\n",
      "47\n",
      "72\n",
      "43\n",
      "90\n",
      "25\n",
      "77\n",
      "32\n",
      "73\n",
      "41\n",
      "70\n",
      "39\n",
      "69\n",
      "63\n",
      "65\n",
      "41\n",
      "93\n",
      "47\n",
      "67\n",
      "32\n",
      "100\n",
      "53\n",
      "79\n",
      "22\n",
      "87\n",
      "28\n",
      "90\n",
      "46\n",
      "65\n",
      "54\n",
      "82\n",
      "40\n",
      "89\n",
      "51\n",
      "93\n",
      "56\n",
      "86\n",
      "34\n",
      "90\n",
      "30\n",
      "98\n",
      "30\n",
      "74\n",
      "32\n",
      "84\n",
      "49\n",
      "81\n",
      "53\n",
      "70\n",
      "21\n",
      "84\n",
      "16\n",
      "72\n",
      "37\n",
      "99\n",
      "46\n",
      "100\n",
      "30\n",
      "92\n",
      "50\n",
      "77\n",
      "24\n",
      "69\n",
      "39\n",
      "71\n",
      "30\n",
      "76\n",
      "32\n",
      "88\n",
      "55\n",
      "67\n",
      "33\n",
      "87\n",
      "37\n",
      "90\n",
      "40\n",
      "74\n",
      "20\n",
      "78\n",
      "47\n",
      "65\n",
      "53\n",
      "68\n",
      "34\n",
      "95\n",
      "61\n",
      "98\n",
      "50\n",
      "81\n",
      "18\n",
      "73\n",
      "27\n",
      "90\n",
      "53\n",
      "66\n",
      "46\n",
      "75\n",
      "18\n",
      "77\n",
      "16\n",
      "65\n",
      "19\n",
      "89\n",
      "24\n",
      "77\n",
      "16\n",
      "74\n",
      "60\n",
      "86\n",
      "56\n",
      "99\n",
      "57\n",
      "72\n",
      "58\n",
      "66\n",
      "28\n",
      "76\n",
      "23\n",
      "96\n",
      "40\n",
      "75\n",
      "19\n",
      "91\n",
      "46\n",
      "66\n",
      "24\n",
      "85\n",
      "59\n",
      "83\n",
      "40\n",
      "84\n",
      "59\n",
      "78\n",
      "64\n",
      "76\n",
      "41\n",
      "99\n",
      "61\n",
      "77\n",
      "50\n",
      "70\n",
      "34\n",
      "74\n",
      "17\n",
      "99\n",
      "52\n",
      "72\n",
      "46\n",
      "88\n",
      "23\n",
      "65\n",
      "25\n",
      "65\n",
      "28\n",
      "95\n",
      "21\n",
      "89\n",
      "59\n",
      "92\n",
      "46\n",
      "90\n",
      "59\n",
      "72\n",
      "20\n",
      "67\n",
      "25\n",
      "89\n",
      "19\n",
      "77\n",
      "40\n",
      "74\n",
      "15\n",
      "76\n",
      "35\n",
      "74\n",
      "34\n",
      "75\n",
      "35\n",
      "78\n",
      "23\n",
      "67\n",
      "35\n",
      "85\n",
      "44\n",
      "95\n",
      "52\n",
      "99\n",
      "29\n",
      "81\n",
      "21\n",
      "97\n",
      "54\n",
      "76\n",
      "36\n",
      "84\n",
      "41\n",
      "65\n",
      "30\n",
      "94\n",
      "22\n",
      "85\n",
      "53\n",
      "66\n",
      "25\n",
      "91\n",
      "16\n",
      "90\n",
      "48\n",
      "65\n",
      "50\n",
      "65\n",
      "40\n",
      "80\n",
      "20\n",
      "92\n",
      "43\n",
      "92\n",
      "53\n",
      "94\n",
      "38\n",
      "92\n",
      "35\n",
      "83\n",
      "18\n",
      "67\n",
      "63\n",
      "65\n",
      "32\n",
      "97\n",
      "39\n",
      "96\n",
      "24\n",
      "71\n",
      "35\n",
      "93\n",
      "14\n",
      "84\n",
      "38\n",
      "80\n",
      "32\n",
      "91\n",
      "55\n",
      "88\n",
      "41\n",
      "75\n",
      "59\n",
      "90\n",
      "62\n",
      "69\n",
      "24\n",
      "79\n",
      "34\n",
      "84\n",
      "47\n",
      "86\n",
      "22\n",
      "95\n",
      "62\n",
      "78\n",
      "20\n",
      "85\n",
      "20\n",
      "75\n",
      "37\n",
      "73\n",
      "18\n",
      "74\n",
      "37\n",
      "76\n",
      "37\n",
      "65\n",
      "37\n",
      "82\n",
      "43\n",
      "73\n",
      "17\n",
      "87\n",
      "49\n",
      "66\n",
      "27\n",
      "90\n",
      "48\n",
      "78\n",
      "43\n",
      "92\n",
      "58\n",
      "88\n",
      "40\n",
      "99\n",
      "21\n",
      "72\n",
      "40\n",
      "90\n",
      "62\n",
      "92\n",
      "16\n",
      "85\n",
      "48\n",
      "85\n",
      "31\n",
      "84\n",
      "33\n",
      "93\n",
      "62\n",
      "69\n",
      "25\n",
      "93\n",
      "59\n",
      "82\n",
      "30\n",
      "85\n",
      "14\n",
      "97\n",
      "62\n",
      "72\n",
      "48\n",
      "73\n",
      "48\n",
      "90\n",
      "15\n",
      "93\n",
      "43\n",
      "78\n",
      "44\n",
      "88\n",
      "58\n",
      "86\n",
      "46\n",
      "74\n",
      "33\n",
      "78\n",
      "14\n",
      "77\n",
      "37\n",
      "78\n",
      "47\n",
      "71\n",
      "16\n",
      "85\n",
      "36\n",
      "100\n",
      "44\n",
      "100\n",
      "23\n",
      "90\n",
      "15\n",
      "74\n",
      "48\n",
      "81\n",
      "49\n",
      "67\n",
      "47\n",
      "85\n",
      "14\n",
      "95\n",
      "27\n",
      "86\n",
      "51\n",
      "74\n",
      "34\n",
      "74\n",
      "61\n",
      "87\n",
      "34\n",
      "82\n",
      "33\n",
      "86\n",
      "50\n",
      "92\n",
      "36\n",
      "78\n",
      "39\n",
      "72\n",
      "56\n",
      "90\n",
      "27\n",
      "97\n",
      "35\n",
      "90\n",
      "29\n",
      "65\n",
      "18\n",
      "85\n",
      "50\n",
      "84\n",
      "17\n",
      "67\n",
      "50\n",
      "76\n",
      "30\n",
      "80\n",
      "64\n",
      "95\n",
      "64\n",
      "99\n",
      "15\n",
      "75\n",
      "56\n",
      "81\n",
      "17\n",
      "72\n",
      "54\n",
      "83\n",
      "39\n",
      "90\n",
      "63\n",
      "68\n",
      "20\n",
      "79\n",
      "16\n",
      "73\n",
      "51\n",
      "94\n",
      "17\n",
      "72\n",
      "60\n",
      "80\n",
      "13\n",
      "76\n",
      "27\n",
      "93\n",
      "30\n",
      "68\n",
      "34\n",
      "82\n",
      "28\n",
      "70\n",
      "53\n",
      "73\n",
      "41\n",
      "99\n",
      "63\n",
      "91\n",
      "34\n",
      "69\n",
      "38\n",
      "95\n",
      "13\n",
      "87\n",
      "13\n",
      "66\n",
      "38\n",
      "75\n",
      "27\n",
      "80\n",
      "35\n",
      "74\n",
      "18\n",
      "68\n",
      "21\n",
      "85\n",
      "18\n",
      "81\n",
      "32\n",
      "65\n",
      "21\n",
      "86\n",
      "26\n",
      "87\n",
      "64\n",
      "95\n",
      "25\n",
      "76\n",
      "55\n",
      "71\n",
      "16\n",
      "92\n",
      "31\n",
      "73\n",
      "46\n",
      "79\n",
      "30\n",
      "91\n",
      "31\n",
      "81\n",
      "48\n",
      "81\n",
      "62\n",
      "83\n",
      "64\n",
      "91\n",
      "42\n",
      "65\n",
      "37\n",
      "95\n",
      "63\n",
      "86\n",
      "26\n",
      "95\n",
      "23\n",
      "91\n",
      "16\n",
      "73\n",
      "27\n",
      "89\n",
      "20\n",
      "75\n",
      "20\n",
      "94\n",
      "22\n",
      "87\n",
      "60\n",
      "100\n",
      "46\n",
      "75\n",
      "59\n",
      "87\n",
      "41\n",
      "94\n",
      "24\n",
      "65\n",
      "47\n",
      "65\n",
      "22\n",
      "87\n",
      "50\n",
      "73\n",
      "38\n",
      "82\n",
      "36\n",
      "79\n",
      "42\n",
      "98\n",
      "14\n",
      "98\n",
      "36\n",
      "76\n",
      "38\n",
      "83\n",
      "55\n",
      "96\n",
      "49\n",
      "97\n",
      "37\n",
      "65\n",
      "26\n",
      "75\n",
      "29\n",
      "80\n",
      "41\n",
      "66\n",
      "64\n",
      "94\n",
      "24\n",
      "84\n",
      "52\n",
      "92\n",
      "26\n",
      "98\n",
      "22\n",
      "83\n",
      "38\n",
      "75\n",
      "23\n",
      "80\n",
      "57\n",
      "70\n",
      "62\n",
      "100\n",
      "55\n",
      "91\n",
      "64\n",
      "97\n",
      "51\n",
      "68\n",
      "35\n",
      "83\n",
      "20\n",
      "69\n",
      "18\n",
      "92\n",
      "39\n",
      "67\n",
      "59\n",
      "73\n",
      "25\n",
      "91\n",
      "28\n",
      "68\n",
      "60\n",
      "69\n",
      "60\n",
      "71\n",
      "34\n",
      "72\n",
      "36\n",
      "71\n",
      "42\n",
      "93\n",
      "49\n",
      "90\n",
      "41\n",
      "84\n",
      "35\n",
      "89\n",
      "46\n",
      "77\n",
      "30\n",
      "95\n",
      "31\n",
      "96\n",
      "15\n",
      "79\n",
      "61\n",
      "95\n",
      "23\n",
      "84\n",
      "24\n",
      "77\n",
      "24\n",
      "97\n",
      "27\n",
      "79\n",
      "54\n",
      "84\n",
      "50\n",
      "99\n",
      "20\n",
      "79\n",
      "52\n",
      "89\n",
      "14\n",
      "89\n",
      "49\n",
      "83\n",
      "16\n",
      "89\n",
      "60\n",
      "100\n",
      "31\n",
      "84\n",
      "34\n",
      "78\n",
      "42\n",
      "82\n",
      "45\n",
      "76\n",
      "19\n",
      "74\n",
      "47\n",
      "67\n",
      "64\n",
      "88\n",
      "55\n",
      "65\n",
      "44\n",
      "71\n",
      "20\n",
      "86\n",
      "26\n",
      "69\n",
      "55\n",
      "95\n",
      "49\n",
      "98\n",
      "35\n",
      "76\n",
      "47\n",
      "80\n",
      "56\n",
      "95\n",
      "57\n",
      "81\n",
      "41\n",
      "91\n",
      "45\n",
      "97\n",
      "19\n",
      "87\n",
      "55\n",
      "84\n",
      "39\n",
      "83\n",
      "15\n",
      "82\n",
      "40\n",
      "82\n",
      "54\n",
      "71\n",
      "38\n",
      "74\n",
      "24\n",
      "68\n",
      "44\n",
      "81\n",
      "36\n",
      "82\n",
      "46\n",
      "77\n",
      "23\n",
      "80\n",
      "59\n",
      "81\n",
      "60\n",
      "78\n",
      "22\n",
      "89\n",
      "24\n",
      "90\n",
      "25\n",
      "68\n",
      "53\n",
      "66\n",
      "19\n",
      "67\n",
      "40\n",
      "69\n",
      "49\n",
      "95\n",
      "35\n",
      "84\n",
      "20\n",
      "87\n",
      "18\n",
      "72\n",
      "16\n",
      "94\n",
      "20\n",
      "73\n",
      "19\n",
      "72\n",
      "43\n",
      "80\n",
      "34\n",
      "98\n",
      "38\n",
      "89\n",
      "35\n",
      "82\n",
      "13\n",
      "87\n",
      "41\n",
      "81\n",
      "16\n",
      "73\n",
      "16\n",
      "73\n",
      "18\n",
      "65\n",
      "39\n",
      "95\n",
      "64\n",
      "85\n",
      "34\n",
      "84\n",
      "14\n",
      "83\n",
      "49\n",
      "96\n",
      "42\n",
      "81\n",
      "41\n",
      "82\n",
      "22\n",
      "72\n",
      "16\n",
      "86\n",
      "40\n",
      "74\n",
      "35\n",
      "80\n",
      "58\n",
      "85\n",
      "60\n",
      "97\n",
      "40\n",
      "98\n",
      "26\n",
      "80\n",
      "24\n",
      "81\n",
      "26\n",
      "80\n",
      "18\n",
      "83\n",
      "26\n",
      "99\n",
      "53\n",
      "74\n",
      "47\n",
      "81\n",
      "19\n",
      "100\n",
      "17\n",
      "84\n",
      "15\n",
      "88\n",
      "38\n",
      "90\n",
      "20\n",
      "94\n",
      "28\n",
      "72\n",
      "27\n",
      "98\n",
      "35\n",
      "91\n",
      "54\n",
      "76\n",
      "19\n",
      "97\n",
      "23\n",
      "82\n",
      "57\n",
      "88\n",
      "34\n",
      "100\n",
      "61\n",
      "86\n",
      "28\n",
      "89\n",
      "61\n",
      "77\n",
      "38\n",
      "67\n",
      "44\n",
      "67\n",
      "55\n",
      "98\n",
      "32\n",
      "75\n",
      "52\n",
      "79\n",
      "32\n",
      "87\n",
      "31\n",
      "100\n",
      "48\n",
      "97\n",
      "31\n",
      "78\n",
      "58\n",
      "78\n",
      "20\n",
      "97\n",
      "29\n",
      "94\n",
      "28\n",
      "82\n",
      "17\n",
      "74\n",
      "20\n",
      "86\n",
      "54\n",
      "89\n",
      "25\n",
      "78\n",
      "42\n",
      "98\n",
      "47\n",
      "80\n",
      "19\n",
      "68\n",
      "50\n",
      "75\n",
      "29\n",
      "71\n",
      "45\n",
      "80\n",
      "24\n",
      "78\n",
      "50\n",
      "89\n",
      "58\n",
      "92\n",
      "32\n",
      "75\n",
      "25\n",
      "72\n",
      "61\n",
      "82\n",
      "13\n",
      "68\n",
      "61\n",
      "92\n",
      "32\n",
      "80\n",
      "59\n",
      "92\n",
      "61\n",
      "69\n",
      "41\n",
      "75\n",
      "39\n",
      "83\n",
      "29\n",
      "100\n",
      "29\n",
      "65\n",
      "58\n",
      "69\n",
      "63\n",
      "100\n",
      "62\n",
      "99\n",
      "50\n",
      "94\n",
      "17\n",
      "86\n",
      "49\n",
      "72\n",
      "16\n",
      "83\n",
      "43\n",
      "88\n",
      "20\n",
      "89\n",
      "15\n",
      "96\n",
      "31\n",
      "91\n",
      "43\n",
      "88\n",
      "64\n",
      "87\n",
      "13\n",
      "77\n",
      "35\n",
      "100\n",
      "16\n",
      "65\n",
      "44\n",
      "66\n",
      "58\n",
      "68\n",
      "53\n",
      "68\n",
      "42\n",
      "81\n",
      "15\n",
      "66\n",
      "16\n",
      "76\n",
      "46\n",
      "93\n",
      "34\n",
      "73\n",
      "63\n",
      "82\n",
      "18\n",
      "69\n",
      "38\n",
      "78\n",
      "41\n",
      "94\n",
      "28\n",
      "98\n",
      "25\n",
      "90\n",
      "31\n",
      "69\n",
      "63\n",
      "92\n",
      "19\n",
      "81\n",
      "31\n",
      "69\n",
      "23\n",
      "88\n",
      "24\n",
      "66\n",
      "39\n",
      "65\n",
      "18\n",
      "95\n",
      "37\n",
      "88\n",
      "39\n",
      "89\n",
      "35\n",
      "86\n",
      "30\n",
      "99\n",
      "56\n",
      "94\n",
      "52\n",
      "90\n",
      "31\n",
      "87\n",
      "56\n",
      "86\n",
      "20\n",
      "98\n",
      "13\n",
      "96\n",
      "57\n",
      "91\n",
      "50\n",
      "71\n",
      "14\n",
      "73\n",
      "60\n",
      "73\n",
      "42\n",
      "67\n",
      "38\n",
      "72\n",
      "60\n",
      "77\n",
      "28\n",
      "88\n",
      "41\n",
      "75\n",
      "25\n",
      "73\n",
      "23\n",
      "89\n",
      "61\n",
      "83\n",
      "26\n",
      "98\n",
      "34\n",
      "69\n",
      "30\n",
      "70\n",
      "24\n",
      "93\n",
      "27\n",
      "79\n",
      "18\n",
      "79\n",
      "64\n",
      "80\n",
      "54\n",
      "78\n",
      "51\n",
      "86\n",
      "35\n",
      "75\n",
      "30\n",
      "97\n",
      "23\n",
      "73\n",
      "23\n",
      "68\n",
      "23\n",
      "72\n",
      "13\n",
      "96\n",
      "53\n",
      "85\n",
      "34\n",
      "90\n",
      "47\n",
      "80\n",
      "43\n",
      "99\n",
      "60\n",
      "73\n",
      "54\n",
      "80\n",
      "62\n",
      "83\n",
      "48\n",
      "82\n",
      "27\n",
      "82\n",
      "62\n",
      "99\n",
      "57\n",
      "96\n",
      "16\n",
      "82\n",
      "60\n",
      "84\n",
      "30\n",
      "95\n",
      "16\n",
      "65\n",
      "13\n",
      "74\n",
      "19\n",
      "69\n",
      "18\n",
      "82\n",
      "32\n",
      "96\n",
      "34\n",
      "88\n",
      "45\n",
      "83\n",
      "15\n",
      "82\n",
      "46\n",
      "68\n",
      "35\n",
      "68\n",
      "39\n",
      "66\n",
      "19\n",
      "87\n",
      "22\n",
      "92\n",
      "45\n",
      "88\n",
      "35\n",
      "98\n",
      "57\n",
      "68\n",
      "63\n",
      "93\n",
      "37\n",
      "99\n",
      "22\n",
      "87\n",
      "48\n",
      "91\n",
      "52\n",
      "99\n",
      "22\n",
      "76\n",
      "48\n",
      "88\n",
      "55\n",
      "67\n",
      "42\n",
      "69\n",
      "32\n",
      "95\n",
      "54\n",
      "67\n",
      "15\n",
      "85\n",
      "32\n",
      "76\n",
      "53\n",
      "82\n",
      "40\n",
      "74\n",
      "22\n",
      "76\n",
      "21\n",
      "91\n",
      "27\n",
      "97\n",
      "47\n",
      "96\n",
      "27\n",
      "68\n",
      "18\n",
      "67\n",
      "25\n",
      "71\n",
      "46\n",
      "74\n",
      "29\n",
      "67\n",
      "26\n",
      "94\n",
      "19\n",
      "100\n",
      "54\n",
      "91\n",
      "52\n",
      "70\n",
      "22\n",
      "75\n",
      "42\n",
      "65\n",
      "45\n",
      "71\n",
      "16\n",
      "86\n",
      "59\n",
      "93\n",
      "16\n",
      "92\n",
      "39\n",
      "99\n",
      "38\n",
      "88\n",
      "28\n",
      "93\n",
      "23\n",
      "84\n",
      "56\n",
      "79\n",
      "51\n",
      "74\n",
      "53\n",
      "93\n",
      "30\n",
      "94\n",
      "42\n",
      "76\n",
      "18\n",
      "89\n",
      "33\n",
      "75\n",
      "30\n",
      "100\n",
      "60\n",
      "65\n",
      "28\n",
      "72\n",
      "39\n",
      "88\n",
      "32\n",
      "80\n",
      "39\n",
      "68\n",
      "63\n",
      "78\n",
      "26\n",
      "98\n",
      "50\n",
      "73\n",
      "27\n",
      "85\n",
      "15\n",
      "85\n",
      "63\n",
      "91\n",
      "29\n",
      "82\n",
      "55\n",
      "86\n",
      "31\n",
      "77\n",
      "25\n",
      "88\n",
      "61\n",
      "87\n",
      "38\n",
      "99\n",
      "40\n",
      "83\n",
      "19\n",
      "100\n",
      "15\n",
      "100\n",
      "35\n",
      "73\n",
      "52\n",
      "96\n",
      "32\n",
      "94\n",
      "27\n",
      "86\n",
      "22\n",
      "79\n",
      "40\n",
      "83\n",
      "36\n",
      "66\n",
      "33\n",
      "88\n",
      "40\n",
      "92\n",
      "50\n",
      "89\n",
      "43\n",
      "94\n",
      "44\n",
      "80\n",
      "22\n",
      "68\n",
      "28\n",
      "91\n",
      "17\n",
      "91\n",
      "48\n",
      "93\n",
      "23\n",
      "82\n",
      "40\n",
      "92\n",
      "20\n",
      "70\n",
      "54\n",
      "66\n",
      "64\n",
      "84\n",
      "51\n",
      "76\n",
      "63\n",
      "93\n",
      "52\n",
      "92\n",
      "40\n",
      "86\n",
      "17\n",
      "81\n",
      "36\n",
      "77\n",
      "20\n",
      "90\n",
      "59\n",
      "96\n",
      "48\n",
      "85\n",
      "22\n",
      "90\n",
      "33\n",
      "90\n",
      "59\n",
      "70\n",
      "37\n",
      "91\n",
      "56\n",
      "71\n",
      "36\n",
      "99\n",
      "31\n",
      "79\n",
      "45\n",
      "75\n",
      "37\n",
      "77\n",
      "42\n",
      "90\n",
      "46\n",
      "84\n",
      "58\n",
      "93\n",
      "44\n",
      "76\n",
      "59\n",
      "79\n",
      "38\n",
      "91\n",
      "59\n",
      "98\n",
      "49\n",
      "73\n",
      "61\n",
      "73\n",
      "30\n",
      "81\n",
      "33\n",
      "65\n",
      "32\n",
      "93\n",
      "55\n",
      "65\n",
      "32\n",
      "74\n",
      "16\n",
      "88\n",
      "44\n",
      "65\n",
      "35\n",
      "97\n",
      "19\n",
      "87\n",
      "48\n",
      "85\n",
      "38\n",
      "68\n",
      "19\n",
      "96\n",
      "62\n",
      "77\n",
      "33\n",
      "95\n",
      "57\n",
      "83\n",
      "50\n",
      "75\n",
      "40\n",
      "97\n",
      "55\n",
      "98\n",
      "46\n",
      "96\n",
      "49\n",
      "81\n",
      "14\n",
      "98\n",
      "40\n",
      "72\n",
      "27\n",
      "89\n",
      "64\n",
      "87\n",
      "25\n",
      "83\n",
      "56\n",
      "89\n",
      "17\n",
      "100\n",
      "16\n",
      "92\n",
      "42\n",
      "73\n",
      "15\n",
      "73\n",
      "16\n",
      "75\n",
      "58\n",
      "90\n",
      "49\n",
      "86\n",
      "13\n",
      "81\n",
      "56\n",
      "99\n",
      "47\n",
      "86\n",
      "44\n",
      "91\n",
      "46\n",
      "77\n",
      "32\n",
      "100\n",
      "38\n",
      "67\n",
      "24\n",
      "97\n",
      "50\n",
      "84\n",
      "55\n",
      "91\n",
      "45\n",
      "81\n",
      "43\n",
      "84\n",
      "25\n",
      "73\n",
      "61\n",
      "72\n",
      "19\n",
      "74\n",
      "30\n",
      "68\n",
      "39\n",
      "90\n",
      "15\n",
      "98\n",
      "60\n",
      "79\n",
      "32\n",
      "93\n",
      "28\n",
      "100\n",
      "61\n",
      "73\n",
      "64\n",
      "100\n",
      "50\n",
      "72\n",
      "38\n",
      "92\n",
      "37\n",
      "94\n",
      "39\n",
      "97\n",
      "24\n",
      "70\n",
      "13\n",
      "77\n",
      "56\n",
      "71\n",
      "39\n",
      "71\n",
      "57\n",
      "67\n",
      "17\n",
      "94\n",
      "13\n",
      "89\n",
      "42\n",
      "95\n",
      "13\n",
      "98\n",
      "51\n",
      "82\n",
      "51\n",
      "88\n",
      "36\n",
      "80\n",
      "45\n",
      "71\n",
      "19\n",
      "95\n",
      "46\n",
      "73\n",
      "30\n",
      "65\n",
      "23\n",
      "70\n",
      "63\n",
      "68\n",
      "64\n",
      "94\n",
      "53\n",
      "99\n",
      "63\n",
      "76\n",
      "59\n",
      "75\n",
      "18\n",
      "91\n",
      "64\n",
      "85\n",
      "51\n",
      "81\n",
      "26\n",
      "69\n",
      "57\n",
      "92\n",
      "49\n",
      "74\n",
      "44\n",
      "87\n",
      "49\n",
      "72\n",
      "59\n",
      "84\n",
      "32\n",
      "79\n",
      "14\n",
      "81\n",
      "19\n",
      "73\n",
      "41\n",
      "97\n",
      "59\n",
      "93\n",
      "14\n",
      "67\n",
      "21\n",
      "69\n",
      "34\n",
      "76\n",
      "29\n",
      "65\n",
      "25\n",
      "67\n",
      "48\n",
      "83\n",
      "43\n",
      "85\n",
      "48\n",
      "72\n",
      "29\n",
      "100\n",
      "26\n",
      "95\n",
      "28\n",
      "81\n",
      "49\n",
      "86\n",
      "28\n",
      "92\n",
      "22\n",
      "91\n",
      "59\n",
      "73\n",
      "37\n",
      "92\n",
      "44\n",
      "89\n",
      "19\n",
      "71\n",
      "26\n",
      "98\n",
      "38\n",
      "84\n",
      "14\n",
      "82\n",
      "42\n",
      "83\n",
      "27\n",
      "88\n",
      "16\n",
      "90\n",
      "42\n",
      "68\n",
      "42\n",
      "99\n",
      "48\n",
      "97\n",
      "60\n",
      "87\n",
      "25\n",
      "73\n",
      "57\n",
      "81\n",
      "22\n",
      "100\n",
      "35\n",
      "76\n",
      "22\n",
      "79\n",
      "23\n",
      "87\n",
      "56\n",
      "69\n",
      "31\n",
      "82\n",
      "37\n",
      "66\n",
      "25\n",
      "70\n",
      "32\n",
      "73\n",
      "39\n",
      "71\n",
      "59\n",
      "92\n",
      "31\n",
      "65\n",
      "13\n",
      "99\n",
      "54\n",
      "67\n",
      "63\n",
      "100\n",
      "21\n",
      "88\n",
      "15\n",
      "80\n",
      "33\n",
      "94\n",
      "15\n",
      "72\n",
      "51\n",
      "89\n",
      "42\n",
      "98\n",
      "32\n",
      "95\n",
      "62\n",
      "71\n",
      "19\n",
      "95\n",
      "22\n",
      "70\n",
      "43\n",
      "82\n",
      "23\n",
      "91\n",
      "26\n",
      "77\n",
      "13\n",
      "91\n",
      "27\n",
      "99\n",
      "26\n",
      "97\n",
      "39\n",
      "92\n",
      "55\n",
      "79\n",
      "17\n",
      "91\n",
      "59\n",
      "73\n",
      "39\n",
      "71\n",
      "16\n",
      "93\n",
      "19\n",
      "79\n",
      "58\n",
      "76\n",
      "35\n",
      "88\n",
      "63\n",
      "99\n",
      "25\n",
      "66\n",
      "51\n",
      "87\n",
      "19\n",
      "67\n",
      "24\n",
      "96\n",
      "38\n",
      "70\n",
      "20\n",
      "81\n",
      "28\n",
      "80\n",
      "19\n",
      "84\n",
      "63\n",
      "88\n",
      "44\n",
      "68\n",
      "36\n",
      "94\n",
      "59\n",
      "95\n",
      "18\n",
      "98\n",
      "15\n",
      "91\n",
      "38\n",
      "65\n",
      "26\n",
      "85\n",
      "21\n",
      "73\n",
      "58\n",
      "89\n",
      "28\n",
      "78\n",
      "48\n",
      "79\n",
      "54\n",
      "81\n",
      "35\n",
      "98\n",
      "43\n",
      "98\n",
      "31\n",
      "88\n",
      "45\n",
      "100\n",
      "33\n",
      "86\n",
      "29\n",
      "90\n",
      "52\n",
      "92\n",
      "13\n",
      "70\n",
      "64\n",
      "79\n",
      "43\n",
      "75\n",
      "57\n",
      "65\n",
      "41\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "for i in train_samples:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f5441",
   "metadata": {},
   "source": [
    "This is what the train_labels look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741b4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in train_labels:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84be618",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "We now convert both lists into numpy arrays due to what we discussed the fit() function expects, and we then shuffle the arrays to remove any order that was imposed on the data during the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb777d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels, train_samples = shuffle(train_labels, train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed820583",
   "metadata": {},
   "source": [
    "In this form, we now have the ability to pass the data to the model because it is now in the required format, however, before doing that, we'll first scale the data down to a range from 0 to 1.\n",
    "\n",
    "We'll use **scikit-learn's MinMaxScaler class** to scale all of the data down from a scale ranging from 13 to 100 to be on a scale from 0 to 1.\n",
    "\n",
    "We reshape the data as a technical requirement just since the **fit_transform()** function doesn't accept 1D data by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8787b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f53304",
   "metadata": {},
   "source": [
    "Now that the data has been scaled, let's iterate over the scaled data to see what it looks like now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a90e9b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8045977]\n",
      "[0.02298851]\n",
      "[0.66666667]\n",
      "[0.63218391]\n",
      "[0.2183908]\n",
      "[0.75862069]\n",
      "[0.37931034]\n",
      "[0.24137931]\n",
      "[0.93103448]\n",
      "[0.54022989]\n",
      "[0.22988506]\n",
      "[0.86206897]\n",
      "[0.70114943]\n",
      "[0.34482759]\n",
      "[0.72413793]\n",
      "[0.7816092]\n",
      "[0.37931034]\n",
      "[0.51724138]\n",
      "[0.59770115]\n",
      "[0.56321839]\n",
      "[0.02298851]\n",
      "[0.59770115]\n",
      "[0.55172414]\n",
      "[0.66666667]\n",
      "[0.89655172]\n",
      "[0.02298851]\n",
      "[0.6091954]\n",
      "[0.03448276]\n",
      "[0.8045977]\n",
      "[0.77011494]\n",
      "[0.63218391]\n",
      "[0.68965517]\n",
      "[0.14942529]\n",
      "[0.64367816]\n",
      "[0.45977011]\n",
      "[0.71264368]\n",
      "[0.62068966]\n",
      "[0.12643678]\n",
      "[0.89655172]\n",
      "[0.87356322]\n",
      "[0.6091954]\n",
      "[0.81609195]\n",
      "[0.35632184]\n",
      "[0.56321839]\n",
      "[0.96551724]\n",
      "[0.93103448]\n",
      "[0.1954023]\n",
      "[0.20689655]\n",
      "[0.79310345]\n",
      "[0.85057471]\n",
      "[0.63218391]\n",
      "[0.44827586]\n",
      "[0.62068966]\n",
      "[0.2183908]\n",
      "[0.33333333]\n",
      "[0.24137931]\n",
      "[0.85057471]\n",
      "[0.96551724]\n",
      "[0.2183908]\n",
      "[0.82758621]\n",
      "[0.]\n",
      "[0.50574713]\n",
      "[0.49425287]\n",
      "[0.98850575]\n",
      "[0.91954023]\n",
      "[0.94252874]\n",
      "[0.1954023]\n",
      "[0.75862069]\n",
      "[0.63218391]\n",
      "[0.17241379]\n",
      "[0.6091954]\n",
      "[0.04597701]\n",
      "[0.05747126]\n",
      "[1.]\n",
      "[0.08045977]\n",
      "[0.96551724]\n",
      "[0.5862069]\n",
      "[0.74712644]\n",
      "[0.91954023]\n",
      "[0.90804598]\n",
      "[0.83908046]\n",
      "[0.70114943]\n",
      "[0.66666667]\n",
      "[0.25287356]\n",
      "[0.71264368]\n",
      "[0.52873563]\n",
      "[0.34482759]\n",
      "[0.26436782]\n",
      "[0.62068966]\n",
      "[0.70114943]\n",
      "[0.96551724]\n",
      "[0.25287356]\n",
      "[0.]\n",
      "[0.72413793]\n",
      "[0.27586207]\n",
      "[0.77011494]\n",
      "[0.06896552]\n",
      "[0.87356322]\n",
      "[0.79310345]\n",
      "[0.06896552]\n",
      "[0.97701149]\n",
      "[0.67816092]\n",
      "[0.49425287]\n",
      "[0.93103448]\n",
      "[0.05747126]\n",
      "[0.14942529]\n",
      "[0.13793103]\n",
      "[0.3908046]\n",
      "[0.90804598]\n",
      "[0.81609195]\n",
      "[0.1954023]\n",
      "[0.24137931]\n",
      "[0.65517241]\n",
      "[0.35632184]\n",
      "[0.93103448]\n",
      "[0.34482759]\n",
      "[0.67816092]\n",
      "[0.8045977]\n",
      "[0.20689655]\n",
      "[0.27586207]\n",
      "[0.86206897]\n",
      "[0.28735632]\n",
      "[0.86206897]\n",
      "[0.13793103]\n",
      "[0.95402299]\n",
      "[0.86206897]\n",
      "[0.77011494]\n",
      "[0.62068966]\n",
      "[0.26436782]\n",
      "[0.13793103]\n",
      "[0.73563218]\n",
      "[0.43678161]\n",
      "[0.03448276]\n",
      "[0.03448276]\n",
      "[0.24137931]\n",
      "[0.14942529]\n",
      "[0.63218391]\n",
      "[1.]\n",
      "[0.73563218]\n",
      "[0.74712644]\n",
      "[0.32183908]\n",
      "[0.72413793]\n",
      "[0.57471264]\n",
      "[0.25287356]\n",
      "[0.33333333]\n",
      "[0.72413793]\n",
      "[0.03448276]\n",
      "[0.98850575]\n",
      "[0.98850575]\n",
      "[0.13793103]\n",
      "[0.63218391]\n",
      "[0.68965517]\n",
      "[0.97701149]\n",
      "[0.88505747]\n",
      "[0.32183908]\n",
      "[0.1954023]\n",
      "[0.24137931]\n",
      "[0.96551724]\n",
      "[0.37931034]\n",
      "[0.77011494]\n",
      "[1.]\n",
      "[0.26436782]\n",
      "[0.97701149]\n",
      "[0.81609195]\n",
      "[0.74712644]\n",
      "[0.05747126]\n",
      "[0.27586207]\n",
      "[0.66666667]\n",
      "[0.16091954]\n",
      "[0.70114943]\n",
      "[0.04597701]\n",
      "[0.45977011]\n",
      "[0.42528736]\n",
      "[0.29885057]\n",
      "[0.94252874]\n",
      "[0.5862069]\n",
      "[0.8045977]\n",
      "[0.18390805]\n",
      "[0.59770115]\n",
      "[0.65517241]\n",
      "[0.52873563]\n",
      "[0.98850575]\n",
      "[0.56321839]\n",
      "[0.03448276]\n",
      "[0.10344828]\n",
      "[0.31034483]\n",
      "[0.86206897]\n",
      "[0.87356322]\n",
      "[0.14942529]\n",
      "[0.71264368]\n",
      "[0.79310345]\n",
      "[0.34482759]\n",
      "[0.82758621]\n",
      "[0.5862069]\n",
      "[0.94252874]\n",
      "[0.20689655]\n",
      "[0.08045977]\n",
      "[0.97701149]\n",
      "[0.50574713]\n",
      "[0.24137931]\n",
      "[0.85057471]\n",
      "[0.95402299]\n",
      "[0.25287356]\n",
      "[0.70114943]\n",
      "[0.63218391]\n",
      "[0.68965517]\n",
      "[0.63218391]\n",
      "[0.29885057]\n",
      "[0.81609195]\n",
      "[0.]\n",
      "[0.63218391]\n",
      "[0.88505747]\n",
      "[0.42528736]\n",
      "[0.27586207]\n",
      "[0.93103448]\n",
      "[0.91954023]\n",
      "[0.79310345]\n",
      "[0.45977011]\n",
      "[0.47126437]\n",
      "[0.98850575]\n",
      "[0.73563218]\n",
      "[0.71264368]\n",
      "[0.62068966]\n",
      "[0.17241379]\n",
      "[0.44827586]\n",
      "[0.72413793]\n",
      "[0.04597701]\n",
      "[0.24137931]\n",
      "[0.48275862]\n",
      "[0.06896552]\n",
      "[0.27586207]\n",
      "[0.56321839]\n",
      "[0.26436782]\n",
      "[0.97701149]\n",
      "[0.50574713]\n",
      "[0.97701149]\n",
      "[0.54022989]\n",
      "[0.6091954]\n",
      "[0.11494253]\n",
      "[0.28735632]\n",
      "[0.8045977]\n",
      "[0.29885057]\n",
      "[0.17241379]\n",
      "[0.55172414]\n",
      "[0.3908046]\n",
      "[0.86206897]\n",
      "[0.05747126]\n",
      "[0.90804598]\n",
      "[0.54022989]\n",
      "[0.05747126]\n",
      "[0.8045977]\n",
      "[0.94252874]\n",
      "[0.42528736]\n",
      "[0.40229885]\n",
      "[0.43678161]\n",
      "[0.68965517]\n",
      "[0.66666667]\n",
      "[0.81609195]\n",
      "[0.3908046]\n",
      "[0.40229885]\n",
      "[0.93103448]\n",
      "[0.77011494]\n",
      "[0.29885057]\n",
      "[0.28735632]\n",
      "[0.34482759]\n",
      "[0.66666667]\n",
      "[0.59770115]\n",
      "[0.95402299]\n",
      "[0.08045977]\n",
      "[0.74712644]\n",
      "[0.75862069]\n",
      "[0.43678161]\n",
      "[0.2183908]\n",
      "[0.14942529]\n",
      "[0.6091954]\n",
      "[0.72413793]\n",
      "[0.67816092]\n",
      "[0.89655172]\n",
      "[0.72413793]\n",
      "[0.45977011]\n",
      "[0.8045977]\n",
      "[0.04597701]\n",
      "[0.74712644]\n",
      "[0.87356322]\n",
      "[0.40229885]\n",
      "[0.63218391]\n",
      "[0.88505747]\n",
      "[0.05747126]\n",
      "[0.91954023]\n",
      "[0.11494253]\n",
      "[0.31034483]\n",
      "[0.79310345]\n",
      "[0.44827586]\n",
      "[0.10344828]\n",
      "[0.45977011]\n",
      "[0.94252874]\n",
      "[0.10344828]\n",
      "[0.04597701]\n",
      "[0.24137931]\n",
      "[0.77011494]\n",
      "[0.44827586]\n",
      "[0.33333333]\n",
      "[0.09195402]\n",
      "[0.]\n",
      "[0.7816092]\n",
      "[0.03448276]\n",
      "[0.35632184]\n",
      "[0.73563218]\n",
      "[0.48275862]\n",
      "[0.70114943]\n",
      "[0.65517241]\n",
      "[0.3908046]\n",
      "[0.8045977]\n",
      "[0.12643678]\n",
      "[0.2183908]\n",
      "[0.72413793]\n",
      "[0.11494253]\n",
      "[0.72413793]\n",
      "[0.91954023]\n",
      "[0.05747126]\n",
      "[0.37931034]\n",
      "[0.42528736]\n",
      "[0.08045977]\n",
      "[1.]\n",
      "[0.7816092]\n",
      "[0.67816092]\n",
      "[0.83908046]\n",
      "[0.89655172]\n",
      "[0.89655172]\n",
      "[0.81609195]\n",
      "[0.3908046]\n",
      "[0.70114943]\n",
      "[0.64367816]\n",
      "[0.09195402]\n",
      "[0.35632184]\n",
      "[0.35632184]\n",
      "[0.26436782]\n",
      "[0.74712644]\n",
      "[1.]\n",
      "[0.7816092]\n",
      "[0.88505747]\n",
      "[0.8045977]\n",
      "[0.65517241]\n",
      "[0.75862069]\n",
      "[0.2183908]\n",
      "[0.70114943]\n",
      "[0.95402299]\n",
      "[0.09195402]\n",
      "[0.64367816]\n",
      "[0.14942529]\n",
      "[0.04597701]\n",
      "[0.63218391]\n",
      "[0.96551724]\n",
      "[0.65517241]\n",
      "[0.5862069]\n",
      "[0.94252874]\n",
      "[0.05747126]\n",
      "[0.75862069]\n",
      "[0.59770115]\n",
      "[0.6091954]\n",
      "[0.03448276]\n",
      "[0.45977011]\n",
      "[0.68965517]\n",
      "[0.86206897]\n",
      "[1.]\n",
      "[0.94252874]\n",
      "[0.97701149]\n",
      "[0.42528736]\n",
      "[0.52873563]\n",
      "[0.3908046]\n",
      "[0.98850575]\n",
      "[0.29885057]\n",
      "[0.27586207]\n",
      "[0.74712644]\n",
      "[0.]\n",
      "[0.68965517]\n",
      "[0.83908046]\n",
      "[0.33333333]\n",
      "[0.24137931]\n",
      "[0.74712644]\n",
      "[0.85057471]\n",
      "[0.64367816]\n",
      "[0.28735632]\n",
      "[0.12643678]\n",
      "[0.86206897]\n",
      "[0.91954023]\n",
      "[0.10344828]\n",
      "[0.59770115]\n",
      "[0.90804598]\n",
      "[0.62068966]\n",
      "[0.85057471]\n",
      "[0.10344828]\n",
      "[0.24137931]\n",
      "[0.10344828]\n",
      "[0.62068966]\n",
      "[0.83908046]\n",
      "[0.98850575]\n",
      "[0.49425287]\n",
      "[0.54022989]\n",
      "[0.79310345]\n",
      "[0.59770115]\n",
      "[0.89655172]\n",
      "[0.72413793]\n",
      "[0.54022989]\n",
      "[0.64367816]\n",
      "[0.44827586]\n",
      "[0.71264368]\n",
      "[0.90804598]\n",
      "[0.7816092]\n",
      "[0.16091954]\n",
      "[0.13793103]\n",
      "[0.75862069]\n",
      "[0.88505747]\n",
      "[0.85057471]\n",
      "[0.97701149]\n",
      "[0.95402299]\n",
      "[0.1954023]\n",
      "[0.95402299]\n",
      "[0.97701149]\n",
      "[0.95402299]\n",
      "[0.33333333]\n",
      "[0.94252874]\n",
      "[0.48275862]\n",
      "[0.35632184]\n",
      "[0.29885057]\n",
      "[0.13793103]\n",
      "[0.98850575]\n",
      "[0.68965517]\n",
      "[0.12643678]\n",
      "[0.87356322]\n",
      "[0.97701149]\n",
      "[0.1954023]\n",
      "[0.35632184]\n",
      "[0.4137931]\n",
      "[0.57471264]\n",
      "[0.4137931]\n",
      "[0.8045977]\n",
      "[0.31034483]\n",
      "[0.02298851]\n",
      "[0.32183908]\n",
      "[0.01149425]\n",
      "[0.77011494]\n",
      "[0.75862069]\n",
      "[0.06896552]\n",
      "[0.14942529]\n",
      "[0.95402299]\n",
      "[0.16091954]\n",
      "[0.88505747]\n",
      "[0.71264368]\n",
      "[0.4137931]\n",
      "[0.06896552]\n",
      "[0.74712644]\n",
      "[0.64367816]\n",
      "[0.04597701]\n",
      "[0.90804598]\n",
      "[0.77011494]\n",
      "[0.11494253]\n",
      "[0.03448276]\n",
      "[0.42528736]\n",
      "[0.34482759]\n",
      "[0.72413793]\n",
      "[0.20689655]\n",
      "[0.85057471]\n",
      "[0.91954023]\n",
      "[0.8045977]\n",
      "[0.83908046]\n",
      "[0.16091954]\n",
      "[0.72413793]\n",
      "[0.88505747]\n",
      "[0.16091954]\n",
      "[0.64367816]\n",
      "[0.73563218]\n",
      "[0.73563218]\n",
      "[0.27586207]\n",
      "[0.86206897]\n",
      "[0.02298851]\n",
      "[0.67816092]\n",
      "[0.18390805]\n",
      "[0.03448276]\n",
      "[0.7816092]\n",
      "[0.10344828]\n",
      "[0.59770115]\n",
      "[0.3908046]\n",
      "[0.73563218]\n",
      "[0.34482759]\n",
      "[0.31034483]\n",
      "[0.87356322]\n",
      "[0.51724138]\n",
      "[0.45977011]\n",
      "[0.55172414]\n",
      "[0.8045977]\n",
      "[0.52873563]\n",
      "[0.89655172]\n",
      "[1.]\n",
      "[0.05747126]\n",
      "[0.50574713]\n",
      "[0.85057471]\n",
      "[0.02298851]\n",
      "[0.71264368]\n",
      "[0.95402299]\n",
      "[0.31034483]\n",
      "[0.94252874]\n",
      "[0.97701149]\n",
      "[1.]\n",
      "[0.51724138]\n",
      "[0.66666667]\n",
      "[0.68965517]\n",
      "[0.94252874]\n",
      "[0.7816092]\n",
      "[0.93103448]\n",
      "[0.57471264]\n",
      "[0.86206897]\n",
      "[0.77011494]\n",
      "[0.83908046]\n",
      "[0.11494253]\n",
      "[0.89655172]\n",
      "[0.75862069]\n",
      "[0.72413793]\n",
      "[0.5862069]\n",
      "[0.90804598]\n",
      "[0.7816092]\n",
      "[0.89655172]\n",
      "[0.93103448]\n",
      "[0.36781609]\n",
      "[0.6091954]\n",
      "[0.37931034]\n",
      "[0.63218391]\n",
      "[0.34482759]\n",
      "[0.79310345]\n",
      "[0.16091954]\n",
      "[0.88505747]\n",
      "[0.2183908]\n",
      "[0.96551724]\n",
      "[0.16091954]\n",
      "[0.68965517]\n",
      "[0.83908046]\n",
      "[0.43678161]\n",
      "[0.4137931]\n",
      "[0.56321839]\n",
      "[0.02298851]\n",
      "[0.28735632]\n",
      "[0.3908046]\n",
      "[0.25287356]\n",
      "[0.13793103]\n",
      "[0.71264368]\n",
      "[0.59770115]\n",
      "[0.27586207]\n",
      "[0.4137931]\n",
      "[0.4137931]\n",
      "[0.13793103]\n",
      "[0.29885057]\n",
      "[0.94252874]\n",
      "[0.45977011]\n",
      "[0.71264368]\n",
      "[0.17241379]\n",
      "[0.47126437]\n",
      "[0.45977011]\n",
      "[0.25287356]\n",
      "[0.03448276]\n",
      "[0.10344828]\n",
      "[0.06896552]\n",
      "[0.08045977]\n",
      "[0.12643678]\n",
      "[0.1954023]\n",
      "[0.12643678]\n",
      "[0.71264368]\n",
      "[0.11494253]\n",
      "[0.91954023]\n",
      "[0.47126437]\n",
      "[0.51724138]\n",
      "[0.06896552]\n",
      "[0.97701149]\n",
      "[1.]\n",
      "[0.10344828]\n",
      "[0.97701149]\n",
      "[0.59770115]\n",
      "[0.66666667]\n",
      "[0.42528736]\n",
      "[0.97701149]\n",
      "[0.74712644]\n",
      "[0.31034483]\n",
      "[0.47126437]\n",
      "[0.67816092]\n",
      "[0.14942529]\n",
      "[0.34482759]\n",
      "[0.40229885]\n",
      "[0.79310345]\n",
      "[0.62068966]\n",
      "[0.2183908]\n",
      "[0.95402299]\n",
      "[0.62068966]\n",
      "[0.88505747]\n",
      "[0.27586207]\n",
      "[0.26436782]\n",
      "[0.81609195]\n",
      "[0.89655172]\n",
      "[0.89655172]\n",
      "[0.8045977]\n",
      "[0.29885057]\n",
      "[0.49425287]\n",
      "[0.82758621]\n",
      "[0.87356322]\n",
      "[0.90804598]\n",
      "[0.48275862]\n",
      "[0.33333333]\n",
      "[0.11494253]\n",
      "[0.51724138]\n",
      "[0.8045977]\n",
      "[0.10344828]\n",
      "[0.68965517]\n",
      "[0.54022989]\n",
      "[0.70114943]\n",
      "[0.6091954]\n",
      "[0.50574713]\n",
      "[0.2183908]\n",
      "[0.1954023]\n",
      "[0.36781609]\n",
      "[0.27586207]\n",
      "[0.7816092]\n",
      "[0.90804598]\n",
      "[0.66666667]\n",
      "[0.75862069]\n",
      "[0.20689655]\n",
      "[0.29885057]\n",
      "[0.52873563]\n",
      "[0.65517241]\n",
      "[0.17241379]\n",
      "[0.11494253]\n",
      "[0.22988506]\n",
      "[0.08045977]\n",
      "[0.67816092]\n",
      "[0.72413793]\n",
      "[0.5862069]\n",
      "[0.36781609]\n",
      "[0.96551724]\n",
      "[0.64367816]\n",
      "[0.32183908]\n",
      "[0.02298851]\n",
      "[0.70114943]\n",
      "[0.33333333]\n",
      "[0.65517241]\n",
      "[0.42528736]\n",
      "[0.22988506]\n",
      "[0.90804598]\n",
      "[0.18390805]\n",
      "[0.55172414]\n",
      "[0.75862069]\n",
      "[0.7816092]\n",
      "[0.68965517]\n",
      "[0.18390805]\n",
      "[0.63218391]\n",
      "[0.82758621]\n",
      "[0.35632184]\n",
      "[0.93103448]\n",
      "[0.64367816]\n",
      "[0.28735632]\n",
      "[0.94252874]\n",
      "[0.66666667]\n",
      "[0.10344828]\n",
      "[0.59770115]\n",
      "[0.57471264]\n",
      "[0.17241379]\n",
      "[0.70114943]\n",
      "[0.40229885]\n",
      "[0.05747126]\n",
      "[0.7816092]\n",
      "[0.42528736]\n",
      "[0.88505747]\n",
      "[0.81609195]\n",
      "[0.75862069]\n",
      "[0.90804598]\n",
      "[0.08045977]\n",
      "[0.24137931]\n",
      "[0.8045977]\n",
      "[0.95402299]\n",
      "[0.49425287]\n",
      "[0.02298851]\n",
      "[0.06896552]\n",
      "[0.77011494]\n",
      "[0.83908046]\n",
      "[0.68965517]\n",
      "[0.90804598]\n",
      "[0.73563218]\n",
      "[0.88505747]\n",
      "[0.83908046]\n",
      "[0.7816092]\n",
      "[0.74712644]\n",
      "[0.82758621]\n",
      "[0.82758621]\n",
      "[0.93103448]\n",
      "[0.77011494]\n",
      "[0.34482759]\n",
      "[0.3908046]\n",
      "[0.95402299]\n",
      "[0.62068966]\n",
      "[0.51724138]\n",
      "[0.22988506]\n",
      "[0.56321839]\n",
      "[0.10344828]\n",
      "[0.82758621]\n",
      "[0.94252874]\n",
      "[0.57471264]\n",
      "[0.88505747]\n",
      "[0.5862069]\n",
      "[0.1954023]\n",
      "[0.2183908]\n",
      "[0.2183908]\n",
      "[0.02298851]\n",
      "[0.3908046]\n",
      "[0.65517241]\n",
      "[0.5862069]\n",
      "[0.70114943]\n",
      "[0.86206897]\n",
      "[0.49425287]\n",
      "[0.64367816]\n",
      "[0.10344828]\n",
      "[0.82758621]\n",
      "[0.66666667]\n",
      "[0.83908046]\n",
      "[0.98850575]\n",
      "[0.48275862]\n",
      "[0.65517241]\n",
      "[0.33333333]\n",
      "[0.3908046]\n",
      "[0.17241379]\n",
      "[0.97701149]\n",
      "[0.83908046]\n",
      "[0.57471264]\n",
      "[0.8045977]\n",
      "[0.7816092]\n",
      "[0.4137931]\n",
      "[0.96551724]\n",
      "[0.32183908]\n",
      "[0.88505747]\n",
      "[0.64367816]\n",
      "[0.47126437]\n",
      "[0.68965517]\n",
      "[0.82758621]\n",
      "[0.67816092]\n",
      "[0.62068966]\n",
      "[0.90804598]\n",
      "[0.06896552]\n",
      "[0.73563218]\n",
      "[0.1954023]\n",
      "[0.10344828]\n",
      "[0.93103448]\n",
      "[0.72413793]\n",
      "[0.09195402]\n",
      "[0.42528736]\n",
      "[0.73563218]\n",
      "[0.88505747]\n",
      "[0.7816092]\n",
      "[0.06896552]\n",
      "[0.33333333]\n",
      "[0.59770115]\n",
      "[0.48275862]\n",
      "[0.88505747]\n",
      "[0.4137931]\n",
      "[0.02298851]\n",
      "[0.71264368]\n",
      "[0.87356322]\n",
      "[0.20689655]\n",
      "[0.87356322]\n",
      "[0.25287356]\n",
      "[0.95402299]\n",
      "[0.64367816]\n",
      "[0.13793103]\n",
      "[0.13793103]\n",
      "[0.34482759]\n",
      "[0.06896552]\n",
      "[0.96551724]\n",
      "[0.89655172]\n",
      "[0.98850575]\n",
      "[0.5862069]\n",
      "[0.86206897]\n",
      "[0.48275862]\n",
      "[0.96551724]\n",
      "[0.]\n",
      "[0.52873563]\n",
      "[0.5862069]\n",
      "[0.06896552]\n",
      "[0.97701149]\n",
      "[0.97701149]\n",
      "[0.13793103]\n",
      "[0.55172414]\n",
      "[0.6091954]\n",
      "[0.04597701]\n",
      "[0.32183908]\n",
      "[0.52873563]\n",
      "[0.47126437]\n",
      "[0.59770115]\n",
      "[0.1954023]\n",
      "[0.68965517]\n",
      "[0.26436782]\n",
      "[0.85057471]\n",
      "[0.04597701]\n",
      "[0.05747126]\n",
      "[0.97701149]\n",
      "[0.06896552]\n",
      "[0.75862069]\n",
      "[0.83908046]\n",
      "[0.31034483]\n",
      "[0.20689655]\n",
      "[0.90804598]\n",
      "[0.71264368]\n",
      "[0.95402299]\n",
      "[0.66666667]\n",
      "[0.2183908]\n",
      "[0.09195402]\n",
      "[0.42528736]\n",
      "[0.54022989]\n",
      "[0.98850575]\n",
      "[0.37931034]\n",
      "[0.33333333]\n",
      "[0.79310345]\n",
      "[0.89655172]\n",
      "[0.35632184]\n",
      "[0.03448276]\n",
      "[0.88505747]\n",
      "[0.3908046]\n",
      "[0.18390805]\n",
      "[0.22988506]\n",
      "[0.68965517]\n",
      "[0.88505747]\n",
      "[0.3908046]\n",
      "[0.68965517]\n",
      "[0.01149425]\n",
      "[0.64367816]\n",
      "[0.91954023]\n",
      "[0.03448276]\n",
      "[0.02298851]\n",
      "[0.06896552]\n",
      "[0.77011494]\n",
      "[0.17241379]\n",
      "[0.26436782]\n",
      "[0.98850575]\n",
      "[0.08045977]\n",
      "[0.54022989]\n",
      "[0.66666667]\n",
      "[0.57471264]\n",
      "[0.97701149]\n",
      "[0.22988506]\n",
      "[0.22988506]\n",
      "[0.68965517]\n",
      "[0.43678161]\n",
      "[0.68965517]\n",
      "[0.29885057]\n",
      "[0.05747126]\n",
      "[0.77011494]\n",
      "[0.79310345]\n",
      "[0.88505747]\n",
      "[0.22988506]\n",
      "[0.35632184]\n",
      "[0.68965517]\n",
      "[0.75862069]\n",
      "[0.75862069]\n",
      "[0.06896552]\n",
      "[0.57471264]\n",
      "[0.13793103]\n",
      "[0.06896552]\n",
      "[0.17241379]\n",
      "[0.73563218]\n",
      "[0.01149425]\n",
      "[0.29885057]\n",
      "[0.02298851]\n",
      "[0.37931034]\n",
      "[0.10344828]\n",
      "[0.95402299]\n",
      "[0.75862069]\n",
      "[0.31034483]\n",
      "[0.]\n",
      "[0.93103448]\n",
      "[0.48275862]\n",
      "[0.09195402]\n",
      "[0.98850575]\n",
      "[0.03448276]\n",
      "[0.81609195]\n",
      "[0.25287356]\n",
      "[0.72413793]\n",
      "[0.57471264]\n",
      "[0.73563218]\n",
      "[0.34482759]\n",
      "[0.90804598]\n",
      "[0.62068966]\n",
      "[0.87356322]\n",
      "[0.11494253]\n",
      "[0.55172414]\n",
      "[0.91954023]\n",
      "[0.93103448]\n",
      "[0.25287356]\n",
      "[0.96551724]\n",
      "[0.33333333]\n",
      "[0.18390805]\n",
      "[0.79310345]\n",
      "[0.7816092]\n",
      "[0.77011494]\n",
      "[0.31034483]\n",
      "[0.25287356]\n",
      "[0.06896552]\n",
      "[0.43678161]\n",
      "[0.12643678]\n",
      "[0.57471264]\n",
      "[0.26436782]\n",
      "[0.93103448]\n",
      "[0.31034483]\n",
      "[0.67816092]\n",
      "[0.37931034]\n",
      "[0.86206897]\n",
      "[0.88505747]\n",
      "[0.4137931]\n",
      "[0.06896552]\n",
      "[0.32183908]\n",
      "[0.36781609]\n",
      "[0.91954023]\n",
      "[0.79310345]\n",
      "[0.63218391]\n",
      "[0.4137931]\n",
      "[0.18390805]\n",
      "[0.79310345]\n",
      "[0.91954023]\n",
      "[0.28735632]\n",
      "[0.02298851]\n",
      "[0.40229885]\n",
      "[0.74712644]\n",
      "[0.47126437]\n",
      "[0.36781609]\n",
      "[0.79310345]\n",
      "[0.35632184]\n",
      "[0.68965517]\n",
      "[0.13793103]\n",
      "[0.96551724]\n",
      "[0.48275862]\n",
      "[0.95402299]\n",
      "[0.8045977]\n",
      "[0.77011494]\n",
      "[0.]\n",
      "[0.37931034]\n",
      "[0.85057471]\n",
      "[0.28735632]\n",
      "[0.20689655]\n",
      "[0.94252874]\n",
      "[0.68965517]\n",
      "[0.79310345]\n",
      "[0.93103448]\n",
      "[0.67816092]\n",
      "[0.57471264]\n",
      "[0.26436782]\n",
      "[0.44827586]\n",
      "[0.79310345]\n",
      "[0.87356322]\n",
      "[0.09195402]\n",
      "[0.88505747]\n",
      "[0.29885057]\n",
      "[0.14942529]\n",
      "[0.52873563]\n",
      "[0.73563218]\n",
      "[0.37931034]\n",
      "[0.95402299]\n",
      "[0.01149425]\n",
      "[0.33333333]\n",
      "[0.7816092]\n",
      "[1.]\n",
      "[0.91954023]\n",
      "[0.85057471]\n",
      "[0.33333333]\n",
      "[0.79310345]\n",
      "[0.82758621]\n",
      "[0.59770115]\n",
      "[0.83908046]\n",
      "[0.8045977]\n",
      "[0.49425287]\n",
      "[0.75862069]\n",
      "[0.81609195]\n",
      "[0.4137931]\n",
      "[0.2183908]\n",
      "[0.52873563]\n",
      "[0.11494253]\n",
      "[0.89655172]\n",
      "[0.82758621]\n",
      "[0.59770115]\n",
      "[1.]\n",
      "[0.05747126]\n",
      "[0.27586207]\n",
      "[0.83908046]\n",
      "[0.48275862]\n",
      "[0.91954023]\n",
      "[1.]\n",
      "[0.28735632]\n",
      "[0.03448276]\n",
      "[0.72413793]\n",
      "[0.66666667]\n",
      "[0.5862069]\n",
      "[0.63218391]\n",
      "[0.16091954]\n",
      "[0.96551724]\n",
      "[0.81609195]\n",
      "[0.91954023]\n",
      "[0.12643678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12643678]\n",
      "[0.08045977]\n",
      "[0.34482759]\n",
      "[0.32183908]\n",
      "[0.36781609]\n",
      "[0.75862069]\n",
      "[0.7816092]\n",
      "[0.88505747]\n",
      "[0.75862069]\n",
      "[0.83908046]\n",
      "[0.43678161]\n",
      "[0.70114943]\n",
      "[0.4137931]\n",
      "[0.2183908]\n",
      "[0.08045977]\n",
      "[1.]\n",
      "[0.50574713]\n",
      "[0.27586207]\n",
      "[0.1954023]\n",
      "[0.74712644]\n",
      "[0.86206897]\n",
      "[0.67816092]\n",
      "[0.62068966]\n",
      "[0.37931034]\n",
      "[0.66666667]\n",
      "[0.98850575]\n",
      "[0.74712644]\n",
      "[0.73563218]\n",
      "[0.17241379]\n",
      "[0.52873563]\n",
      "[0.63218391]\n",
      "[0.02298851]\n",
      "[0.73563218]\n",
      "[0.11494253]\n",
      "[0.04597701]\n",
      "[0.65517241]\n",
      "[0.]\n",
      "[0.56321839]\n",
      "[0.28735632]\n",
      "[0.14942529]\n",
      "[0.87356322]\n",
      "[0.31034483]\n",
      "[0.45977011]\n",
      "[0.66666667]\n",
      "[0.83908046]\n",
      "[0.83908046]\n",
      "[0.98850575]\n",
      "[0.64367816]\n",
      "[0.64367816]\n",
      "[0.5862069]\n",
      "[0.59770115]\n",
      "[0.34482759]\n",
      "[0.54022989]\n",
      "[0.96551724]\n",
      "[0.48275862]\n",
      "[0.75862069]\n",
      "[0.94252874]\n",
      "[0.40229885]\n",
      "[0.54022989]\n",
      "[0.91954023]\n",
      "[0.66666667]\n",
      "[0.03448276]\n",
      "[0.56321839]\n",
      "[0.17241379]\n",
      "[0.28735632]\n",
      "[0.18390805]\n",
      "[0.88505747]\n",
      "[0.97701149]\n",
      "[0.54022989]\n",
      "[0.85057471]\n",
      "[0.65517241]\n",
      "[0.96551724]\n",
      "[0.14942529]\n",
      "[0.09195402]\n",
      "[0.42528736]\n",
      "[0.1954023]\n",
      "[0.33333333]\n",
      "[0.6091954]\n",
      "[0.98850575]\n",
      "[0.50574713]\n",
      "[0.93103448]\n",
      "[0.17241379]\n",
      "[0.72413793]\n",
      "[0.48275862]\n",
      "[0.10344828]\n",
      "[0.36781609]\n",
      "[0.40229885]\n",
      "[0.]\n",
      "[0.09195402]\n",
      "[0.52873563]\n",
      "[0.14942529]\n",
      "[0.85057471]\n",
      "[0.56321839]\n",
      "[0.68965517]\n",
      "[0.06896552]\n",
      "[0.12643678]\n",
      "[0.65517241]\n",
      "[0.55172414]\n",
      "[0.26436782]\n",
      "[0.62068966]\n",
      "[0.24137931]\n",
      "[0.5862069]\n",
      "[0.32183908]\n",
      "[0.11494253]\n",
      "[0.81609195]\n",
      "[0.27586207]\n",
      "[0.93103448]\n",
      "[0.8045977]\n",
      "[0.68965517]\n",
      "[0.90804598]\n",
      "[0.91954023]\n",
      "[0.20689655]\n",
      "[0.43678161]\n",
      "[0.97701149]\n",
      "[0.73563218]\n",
      "[0.88505747]\n",
      "[0.64367816]\n",
      "[0.90804598]\n",
      "[0.87356322]\n",
      "[0.06896552]\n",
      "[0.59770115]\n",
      "[0.29885057]\n",
      "[0.20689655]\n",
      "[0.85057471]\n",
      "[0.36781609]\n",
      "[0.48275862]\n",
      "[0.1954023]\n",
      "[0.24137931]\n",
      "[0.47126437]\n",
      "[0.04597701]\n",
      "[0.82758621]\n",
      "[0.13793103]\n",
      "[0.8045977]\n",
      "[0.55172414]\n",
      "[0.59770115]\n",
      "[0.59770115]\n",
      "[0.71264368]\n",
      "[0.83908046]\n",
      "[0.7816092]\n",
      "[0.13793103]\n",
      "[0.59770115]\n",
      "[0.98850575]\n",
      "[0.98850575]\n",
      "[0.94252874]\n",
      "[0.11494253]\n",
      "[0.]\n",
      "[0.33333333]\n",
      "[0.73563218]\n",
      "[0.72413793]\n",
      "[0.3908046]\n",
      "[0.81609195]\n",
      "[0.56321839]\n",
      "[0.02298851]\n",
      "[0.36781609]\n",
      "[0.08045977]\n",
      "[0.13793103]\n",
      "[0.96551724]\n",
      "[0.62068966]\n",
      "[0.33333333]\n",
      "[0.90804598]\n",
      "[0.02298851]\n",
      "[0.88505747]\n",
      "[0.3908046]\n",
      "[0.42528736]\n",
      "[1.]\n",
      "[0.88505747]\n",
      "[0.89655172]\n",
      "[0.82758621]\n",
      "[0.73563218]\n",
      "[0.35632184]\n",
      "[0.96551724]\n",
      "[0.5862069]\n",
      "[0.62068966]\n",
      "[0.81609195]\n",
      "[0.77011494]\n",
      "[0.20689655]\n",
      "[0.49425287]\n",
      "[0.59770115]\n",
      "[0.72413793]\n",
      "[0.25287356]\n",
      "[0.83908046]\n",
      "[0.12643678]\n",
      "[0.70114943]\n",
      "[0.86206897]\n",
      "[0.16091954]\n",
      "[0.17241379]\n",
      "[0.96551724]\n",
      "[0.72413793]\n",
      "[0.44827586]\n",
      "[0.06896552]\n",
      "[0.81609195]\n",
      "[0.75862069]\n",
      "[0.67816092]\n",
      "[0.96551724]\n",
      "[0.4137931]\n",
      "[0.16091954]\n",
      "[0.8045977]\n",
      "[0.31034483]\n",
      "[0.94252874]\n",
      "[0.42528736]\n",
      "[0.77011494]\n",
      "[0.97701149]\n",
      "[0.86206897]\n",
      "[0.59770115]\n",
      "[0.68965517]\n",
      "[0.01149425]\n",
      "[0.89655172]\n",
      "[0.17241379]\n",
      "[0.32183908]\n",
      "[0.27586207]\n",
      "[0.67816092]\n",
      "[0.55172414]\n",
      "[0.34482759]\n",
      "[0.40229885]\n",
      "[0.49425287]\n",
      "[0.91954023]\n",
      "[0.35632184]\n",
      "[0.47126437]\n",
      "[0.14942529]\n",
      "[0.89655172]\n",
      "[0.40229885]\n",
      "[0.4137931]\n",
      "[0.14942529]\n",
      "[0.33333333]\n",
      "[0.45977011]\n",
      "[0.90804598]\n",
      "[0.25287356]\n",
      "[0.59770115]\n",
      "[0.08045977]\n",
      "[0.6091954]\n",
      "[0.90804598]\n",
      "[0.5862069]\n",
      "[0.98850575]\n",
      "[0.29885057]\n",
      "[0.75862069]\n",
      "[0.96551724]\n",
      "[0.81609195]\n",
      "[0.68965517]\n",
      "[0.22988506]\n",
      "[1.]\n",
      "[0.79310345]\n",
      "[0.42528736]\n",
      "[0.70114943]\n",
      "[0.72413793]\n",
      "[0.91954023]\n",
      "[0.08045977]\n",
      "[0.70114943]\n",
      "[0.08045977]\n",
      "[0.16091954]\n",
      "[0.97701149]\n",
      "[0.98850575]\n",
      "[1.]\n",
      "[0.98850575]\n",
      "[0.51724138]\n",
      "[0.04597701]\n",
      "[0.62068966]\n",
      "[0.20689655]\n",
      "[0.89655172]\n",
      "[0.67816092]\n",
      "[0.68965517]\n",
      "[0.90804598]\n",
      "[0.63218391]\n",
      "[0.01149425]\n",
      "[0.64367816]\n",
      "[0.3908046]\n",
      "[0.13793103]\n",
      "[0.28735632]\n",
      "[0.82758621]\n",
      "[0.24137931]\n",
      "[0.66666667]\n",
      "[0.18390805]\n",
      "[0.55172414]\n",
      "[0.32183908]\n",
      "[0.7816092]\n",
      "[0.08045977]\n",
      "[0.14942529]\n",
      "[0.18390805]\n",
      "[0.70114943]\n",
      "[0.24137931]\n",
      "[0.14942529]\n",
      "[0.08045977]\n",
      "[0.85057471]\n",
      "[0.08045977]\n",
      "[0.67816092]\n",
      "[0.86206897]\n",
      "[0.13793103]\n",
      "[0.7816092]\n",
      "[0.29885057]\n",
      "[0.81609195]\n",
      "[0.28735632]\n",
      "[0.18390805]\n",
      "[0.01149425]\n",
      "[0.25287356]\n",
      "[0.57471264]\n",
      "[0.75862069]\n",
      "[0.03448276]\n",
      "[0.33333333]\n",
      "[0.26436782]\n",
      "[0.01149425]\n",
      "[1.]\n",
      "[0.11494253]\n",
      "[0.10344828]\n",
      "[0.16091954]\n",
      "[0.59770115]\n",
      "[0.32183908]\n",
      "[0.09195402]\n",
      "[0.47126437]\n",
      "[0.13793103]\n",
      "[0.40229885]\n",
      "[0.82758621]\n",
      "[0.]\n",
      "[0.72413793]\n",
      "[0.25287356]\n",
      "[0.18390805]\n",
      "[0.86206897]\n",
      "[0.63218391]\n",
      "[0.25287356]\n",
      "[0.48275862]\n",
      "[0.68965517]\n",
      "[0.11494253]\n",
      "[0.63218391]\n",
      "[0.79310345]\n",
      "[0.2183908]\n",
      "[0.73563218]\n",
      "[0.89655172]\n",
      "[0.14942529]\n",
      "[0.24137931]\n",
      "[0.88505747]\n",
      "[0.87356322]\n",
      "[0.26436782]\n",
      "[0.05747126]\n",
      "[0.71264368]\n",
      "[0.52873563]\n",
      "[0.73563218]\n",
      "[0.96551724]\n",
      "[0.8045977]\n",
      "[0.87356322]\n",
      "[0.17241379]\n",
      "[0.48275862]\n",
      "[0.01149425]\n",
      "[0.65517241]\n",
      "[0.42528736]\n",
      "[0.33333333]\n",
      "[0.37931034]\n",
      "[0.29885057]\n",
      "[0.88505747]\n",
      "[0.89655172]\n",
      "[0.2183908]\n",
      "[0.91954023]\n",
      "[0.03448276]\n",
      "[0.85057471]\n",
      "[0.52873563]\n",
      "[0.71264368]\n",
      "[0.82758621]\n",
      "[0.40229885]\n",
      "[0.98850575]\n",
      "[0.96551724]\n",
      "[0.47126437]\n",
      "[0.68965517]\n",
      "[0.98850575]\n",
      "[0.48275862]\n",
      "[0.7816092]\n",
      "[0.4137931]\n",
      "[0.52873563]\n",
      "[0.44827586]\n",
      "[0.72413793]\n",
      "[0.59770115]\n",
      "[0.47126437]\n",
      "[0.43678161]\n",
      "[0.74712644]\n",
      "[0.16091954]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.79310345]\n",
      "[0.29885057]\n",
      "[0.35632184]\n",
      "[0.8045977]\n",
      "[0.87356322]\n",
      "[0.77011494]\n",
      "[0.66666667]\n",
      "[0.86206897]\n",
      "[0.32183908]\n",
      "[0.10344828]\n",
      "[0.82758621]\n",
      "[0.73563218]\n",
      "[0.79310345]\n",
      "[0.24137931]\n",
      "[0.87356322]\n",
      "[0.70114943]\n",
      "[0.]\n",
      "[0.6091954]\n",
      "[0.70114943]\n",
      "[0.50574713]\n",
      "[0.75862069]\n",
      "[0.04597701]\n",
      "[0.93103448]\n",
      "[0.42528736]\n",
      "[0.54022989]\n",
      "[0.62068966]\n",
      "[0.37931034]\n",
      "[0.91954023]\n",
      "[0.2183908]\n",
      "[0.67816092]\n",
      "[0.97701149]\n",
      "[0.6091954]\n",
      "[0.59770115]\n",
      "[0.68965517]\n",
      "[0.95402299]\n",
      "[0.10344828]\n",
      "[0.87356322]\n",
      "[0.03448276]\n",
      "[0.62068966]\n",
      "[0.87356322]\n",
      "[0.68965517]\n",
      "[0.40229885]\n",
      "[0.40229885]\n",
      "[0.94252874]\n",
      "[0.95402299]\n",
      "[0.89655172]\n",
      "[0.57471264]\n",
      "[0.43678161]\n",
      "[0.]\n",
      "[0.57471264]\n",
      "[0.28735632]\n",
      "[0.32183908]\n",
      "[0.29885057]\n",
      "[0.64367816]\n",
      "[1.]\n",
      "[0.31034483]\n",
      "[0.16091954]\n",
      "[0.49425287]\n",
      "[1.]\n",
      "[0.57471264]\n",
      "[0.35632184]\n",
      "[0.82758621]\n",
      "[0.81609195]\n",
      "[0.72413793]\n",
      "[0.05747126]\n",
      "[0.09195402]\n",
      "[1.]\n",
      "[0.67816092]\n",
      "[0.67816092]\n",
      "[0.40229885]\n",
      "[0.20689655]\n",
      "[0.91954023]\n",
      "[0.97701149]\n",
      "[0.11494253]\n",
      "[0.98850575]\n",
      "[0.31034483]\n",
      "[0.09195402]\n",
      "[0.93103448]\n",
      "[0.81609195]\n",
      "[0.5862069]\n",
      "[0.08045977]\n",
      "[0.28735632]\n",
      "[0.12643678]\n",
      "[0.71264368]\n",
      "[0.22988506]\n",
      "[0.06896552]\n",
      "[0.87356322]\n",
      "[0.90804598]\n",
      "[0.79310345]\n",
      "[0.55172414]\n",
      "[0.06896552]\n",
      "[0.82758621]\n",
      "[0.57471264]\n",
      "[0.2183908]\n",
      "[0.4137931]\n",
      "[0.93103448]\n",
      "[0.49425287]\n",
      "[0.82758621]\n",
      "[0.6091954]\n",
      "[0.97701149]\n",
      "[0.5862069]\n",
      "[0.5862069]\n",
      "[0.55172414]\n",
      "[0.08045977]\n",
      "[0.12643678]\n",
      "[0.77011494]\n",
      "[0.66666667]\n",
      "[0.86206897]\n",
      "[0.43678161]\n",
      "[0.8045977]\n",
      "[0.03448276]\n",
      "[0.62068966]\n",
      "[0.29885057]\n",
      "[0.31034483]\n",
      "[0.37931034]\n",
      "[0.94252874]\n",
      "[0.85057471]\n",
      "[0.1954023]\n",
      "[0.18390805]\n",
      "[0.64367816]\n",
      "[0.82758621]\n",
      "[0.17241379]\n",
      "[0.81609195]\n",
      "[0.2183908]\n",
      "[0.36781609]\n",
      "[0.81609195]\n",
      "[0.13793103]\n",
      "[0.44827586]\n",
      "[0.7816092]\n",
      "[0.97701149]\n",
      "[0.31034483]\n",
      "[0.37931034]\n",
      "[0.86206897]\n",
      "[0.05747126]\n",
      "[0.47126437]\n",
      "[0.49425287]\n",
      "[0.79310345]\n",
      "[0.97701149]\n",
      "[0.26436782]\n",
      "[0.51724138]\n",
      "[0.89655172]\n",
      "[0.5862069]\n",
      "[0.90804598]\n",
      "[0.96551724]\n",
      "[0.48275862]\n",
      "[0.13793103]\n",
      "[0.8045977]\n",
      "[0.59770115]\n",
      "[0.52873563]\n",
      "[0.1954023]\n",
      "[0.16091954]\n",
      "[0.89655172]\n",
      "[0.88505747]\n",
      "[0.89655172]\n",
      "[0.98850575]\n",
      "[0.52873563]\n",
      "[0.27586207]\n",
      "[0.50574713]\n",
      "[0.85057471]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.8045977]\n",
      "[0.22988506]\n",
      "[0.66666667]\n",
      "[0.70114943]\n",
      "[0.10344828]\n",
      "[0.88505747]\n",
      "[0.74712644]\n",
      "[0.86206897]\n",
      "[0.33333333]\n",
      "[0.72413793]\n",
      "[0.56321839]\n",
      "[0.63218391]\n",
      "[0.91954023]\n",
      "[0.98850575]\n",
      "[0.94252874]\n",
      "[0.37931034]\n",
      "[0.88505747]\n",
      "[0.25287356]\n",
      "[0.59770115]\n",
      "[0.05747126]\n",
      "[0.29885057]\n",
      "[0.67816092]\n",
      "[0.95402299]\n",
      "[0.28735632]\n",
      "[0.36781609]\n",
      "[0.52873563]\n",
      "[1.]\n",
      "[0.05747126]\n",
      "[0.2183908]\n",
      "[0.49425287]\n",
      "[0.45977011]\n",
      "[0.96551724]\n",
      "[0.85057471]\n",
      "[0.97701149]\n",
      "[0.98850575]\n",
      "[0.67816092]\n",
      "[0.29885057]\n",
      "[0.28735632]\n",
      "[0.49425287]\n",
      "[0.02298851]\n",
      "[0.59770115]\n",
      "[0.72413793]\n",
      "[0.3908046]\n",
      "[0.34482759]\n",
      "[0.79310345]\n",
      "[0.54022989]\n",
      "[0.6091954]\n",
      "[0.22988506]\n",
      "[0.83908046]\n",
      "[0.25287356]\n",
      "[0.24137931]\n",
      "[0.82758621]\n",
      "[0.65517241]\n",
      "[0.12643678]\n",
      "[0.48275862]\n",
      "[0.90804598]\n",
      "[0.57471264]\n",
      "[0.75862069]\n",
      "[0.28735632]\n",
      "[0.64367816]\n",
      "[0.2183908]\n",
      "[1.]\n",
      "[0.83908046]\n",
      "[0.37931034]\n",
      "[1.]\n",
      "[0.40229885]\n",
      "[0.63218391]\n",
      "[0.94252874]\n",
      "[0.77011494]\n",
      "[0.28735632]\n",
      "[0.97701149]\n",
      "[0.87356322]\n",
      "[0.65517241]\n",
      "[0.68965517]\n",
      "[0.90804598]\n",
      "[0.25287356]\n",
      "[0.68965517]\n",
      "[0.01149425]\n",
      "[0.66666667]\n",
      "[0.81609195]\n",
      "[0.49425287]\n",
      "[0.48275862]\n",
      "[0.79310345]\n",
      "[0.62068966]\n",
      "[0.67816092]\n",
      "[0.6091954]\n",
      "[0.5862069]\n",
      "[0.10344828]\n",
      "[0.16091954]\n",
      "[0.65517241]\n",
      "[0.82758621]\n",
      "[0.03448276]\n",
      "[0.71264368]\n",
      "[0.42528736]\n",
      "[0.10344828]\n",
      "[0.2183908]\n",
      "[0.74712644]\n",
      "[0.8045977]\n",
      "[0.43678161]\n",
      "[0.37931034]\n",
      "[0.77011494]\n",
      "[0.4137931]\n",
      "[0.5862069]\n",
      "[0.94252874]\n",
      "[0.48275862]\n",
      "[0.33333333]\n",
      "[0.42528736]\n",
      "[0.40229885]\n",
      "[0.70114943]\n",
      "[0.93103448]\n",
      "[0.73563218]\n",
      "[0.55172414]\n",
      "[0.26436782]\n",
      "[0.17241379]\n",
      "[0.67816092]\n",
      "[0.7816092]\n",
      "[0.4137931]\n",
      "[0.63218391]\n",
      "[0.10344828]\n",
      "[0.82758621]\n",
      "[0.4137931]\n",
      "[0.31034483]\n",
      "[0.55172414]\n",
      "[0.3908046]\n",
      "[0.24137931]\n",
      "[0.5862069]\n",
      "[0.90804598]\n",
      "[0.03448276]\n",
      "[0.29885057]\n",
      "[0.08045977]\n",
      "[0.93103448]\n",
      "[0.98850575]\n",
      "[0.88505747]\n",
      "[0.5862069]\n",
      "[0.47126437]\n",
      "[0.67816092]\n",
      "[0.14942529]\n",
      "[0.75862069]\n",
      "[0.49425287]\n",
      "[0.67816092]\n",
      "[0.94252874]\n",
      "[0.96551724]\n",
      "[0.70114943]\n",
      "[0.18390805]\n",
      "[0.66666667]\n",
      "[1.]\n",
      "[0.77011494]\n",
      "[0.7816092]\n",
      "[0.36781609]\n",
      "[0.90804598]\n",
      "[0.32183908]\n",
      "[0.52873563]\n",
      "[0.50574713]\n",
      "[0.02298851]\n",
      "[0.05747126]\n",
      "[0.56321839]\n",
      "[0.09195402]\n",
      "[0.16091954]\n",
      "[0.88505747]\n",
      "[0.25287356]\n",
      "[0.24137931]\n",
      "[0.08045977]\n",
      "[0.08045977]\n",
      "[0.81609195]\n",
      "[0.77011494]\n",
      "[0.7816092]\n",
      "[0.94252874]\n",
      "[0.74712644]\n",
      "[0.52873563]\n",
      "[0.25287356]\n",
      "[0.68965517]\n",
      "[0.71264368]\n",
      "[0.51724138]\n",
      "[0.54022989]\n",
      "[0.68965517]\n",
      "[0.25287356]\n",
      "[0.50574713]\n",
      "[0.20689655]\n",
      "[0.63218391]\n",
      "[0.45977011]\n",
      "[0.59770115]\n",
      "[0.96551724]\n",
      "[0.6091954]\n",
      "[0.31034483]\n",
      "[0.51724138]\n",
      "[0.74712644]\n",
      "[0.2183908]\n",
      "[0.83908046]\n",
      "[0.81609195]\n",
      "[0.95402299]\n",
      "[0.33333333]\n",
      "[0.62068966]\n",
      "[0.85057471]\n",
      "[0.85057471]\n",
      "[0.28735632]\n",
      "[0.]\n",
      "[0.32183908]\n",
      "[0.31034483]\n",
      "[0.67816092]\n",
      "[0.86206897]\n",
      "[0.66666667]\n",
      "[0.79310345]\n",
      "[0.87356322]\n",
      "[0.87356322]\n",
      "[0.8045977]\n",
      "[0.83908046]\n",
      "[0.55172414]\n",
      "[0.45977011]\n",
      "[0.16091954]\n",
      "[0.91954023]\n",
      "[0.83908046]\n",
      "[0.64367816]\n",
      "[0.7816092]\n",
      "[0.47126437]\n",
      "[0.7816092]\n",
      "[0.28735632]\n",
      "[0.22988506]\n",
      "[0.8045977]\n",
      "[0.44827586]\n",
      "[0.72413793]\n",
      "[0.97701149]\n",
      "[0.13793103]\n",
      "[0.63218391]\n",
      "[0.98850575]\n",
      "[0.25287356]\n",
      "[0.70114943]\n",
      "[0.17241379]\n",
      "[0.2183908]\n",
      "[0.56321839]\n",
      "[0.59770115]\n",
      "[0.54022989]\n",
      "[0.68965517]\n",
      "[0.48275862]\n",
      "[0.52873563]\n",
      "[0.24137931]\n",
      "[0.63218391]\n",
      "[0.71264368]\n",
      "[0.86206897]\n",
      "[0.87356322]\n",
      "[0.12643678]\n",
      "[0.01149425]\n",
      "[0.87356322]\n",
      "[0.36781609]\n",
      "[0.89655172]\n",
      "[0.77011494]\n",
      "[0.25287356]\n",
      "[0.48275862]\n",
      "[0.89655172]\n",
      "[0.02298851]\n",
      "[0.62068966]\n",
      "[0.87356322]\n",
      "[0.89655172]\n",
      "[0.12643678]\n",
      "[0.81609195]\n",
      "[0.1954023]\n",
      "[0.52873563]\n",
      "[0.96551724]\n",
      "[0.51724138]\n",
      "[0.93103448]\n",
      "[0.26436782]\n",
      "[0.25287356]\n",
      "[1.]\n",
      "[0.64367816]\n",
      "[0.51724138]\n",
      "[0.72413793]\n",
      "[0.75862069]\n",
      "[0.24137931]\n",
      "[0.86206897]\n",
      "[0.8045977]\n",
      "[0.57471264]\n",
      "[1.]\n",
      "[0.12643678]\n",
      "[0.81609195]\n",
      "[0.96551724]\n",
      "[0.90804598]\n",
      "[0.88505747]\n",
      "[0.20689655]\n",
      "[0.50574713]\n",
      "[0.75862069]\n",
      "[0.02298851]\n",
      "[0.73563218]\n",
      "[0.93103448]\n",
      "[0.17241379]\n",
      "[0.5862069]\n",
      "[0.2183908]\n",
      "[0.43678161]\n",
      "[0.12643678]\n",
      "[0.79310345]\n",
      "[0.40229885]\n",
      "[0.54022989]\n",
      "[0.67816092]\n",
      "[0.31034483]\n",
      "[0.3908046]\n",
      "[0.77011494]\n",
      "[0.63218391]\n",
      "[0.86206897]\n",
      "[0.29885057]\n",
      "[0.4137931]\n",
      "[0.05747126]\n",
      "[0.1954023]\n",
      "[0.83908046]\n",
      "[0.70114943]\n",
      "[0.8045977]\n",
      "[0.29885057]\n",
      "[0.59770115]\n",
      "[0.03448276]\n",
      "[0.68965517]\n",
      "[0.71264368]\n",
      "[0.6091954]\n",
      "[0.40229885]\n",
      "[0.93103448]\n",
      "[0.67816092]\n",
      "[0.14942529]\n",
      "[0.89655172]\n",
      "[0.11494253]\n",
      "[0.27586207]\n",
      "[0.27586207]\n",
      "[0.81609195]\n",
      "[0.70114943]\n",
      "[0.26436782]\n",
      "[0.51724138]\n",
      "[0.72413793]\n",
      "[0.66666667]\n",
      "[0.72413793]\n",
      "[0.43678161]\n",
      "[0.29885057]\n",
      "[0.57471264]\n",
      "[0.88505747]\n",
      "[0.89655172]\n",
      "[0.6091954]\n",
      "[0.35632184]\n",
      "[0.96551724]\n",
      "[0.65517241]\n",
      "[0.35632184]\n",
      "[0.97701149]\n",
      "[0.64367816]\n",
      "[0.68965517]\n",
      "[0.59770115]\n",
      "[0.66666667]\n",
      "[0.27586207]\n",
      "[0.97701149]\n",
      "[0.16091954]\n",
      "[0.74712644]\n",
      "[0.71264368]\n",
      "[0.02298851]\n",
      "[0.98850575]\n",
      "[0.81609195]\n",
      "[0.91954023]\n",
      "[0.12643678]\n",
      "[0.90804598]\n",
      "[0.34482759]\n",
      "[0.45977011]\n",
      "[0.90804598]\n",
      "[0.48275862]\n",
      "[0.22988506]\n",
      "[0.05747126]\n",
      "[0.88505747]\n",
      "[0.68965517]\n",
      "[0.16091954]\n",
      "[0.32183908]\n",
      "[0.86206897]\n",
      "[0.26436782]\n",
      "[0.89655172]\n",
      "[0.16091954]\n",
      "[0.90804598]\n",
      "[0.82758621]\n",
      "[0.91954023]\n",
      "[0.01149425]\n",
      "[0.85057471]\n",
      "[0.67816092]\n",
      "[0.57471264]\n",
      "[0.62068966]\n",
      "[0.87356322]\n",
      "[0.67816092]\n",
      "[0.24137931]\n",
      "[0.25287356]\n",
      "[0.28735632]\n",
      "[0.54022989]\n",
      "[0.87356322]\n",
      "[0.88505747]\n",
      "[0.36781609]\n",
      "[0.52873563]\n",
      "[0.25287356]\n",
      "[0.16091954]\n",
      "[0.4137931]\n",
      "[0.89655172]\n",
      "[0.71264368]\n",
      "[0.62068966]\n",
      "[0.08045977]\n",
      "[0.20689655]\n",
      "[0.85057471]\n",
      "[0.45977011]\n",
      "[0.1954023]\n",
      "[0.01149425]\n",
      "[0.64367816]\n",
      "[0.64367816]\n",
      "[0.7816092]\n",
      "[0.32183908]\n",
      "[0.48275862]\n",
      "[0.33333333]\n",
      "[0.93103448]\n",
      "[0.18390805]\n",
      "[0.51724138]\n",
      "[0.68965517]\n",
      "[0.65517241]\n",
      "[0.32183908]\n",
      "[0.04597701]\n",
      "[0.86206897]\n",
      "[0.13793103]\n",
      "[0.22988506]\n",
      "[0.44827586]\n",
      "[0.7816092]\n",
      "[0.10344828]\n",
      "[0.73563218]\n",
      "[0.85057471]\n",
      "[0.97701149]\n",
      "[0.36781609]\n",
      "[0.82758621]\n",
      "[0.04597701]\n",
      "[0.]\n",
      "[0.24137931]\n",
      "[0.89655172]\n",
      "[0.34482759]\n",
      "[0.86206897]\n",
      "[0.81609195]\n",
      "[0.03448276]\n",
      "[0.77011494]\n",
      "[0.26436782]\n",
      "[0.64367816]\n",
      "[0.52873563]\n",
      "[0.59770115]\n",
      "[0.71264368]\n",
      "[0.71264368]\n",
      "[0.05747126]\n",
      "[0.13793103]\n",
      "[0.06896552]\n",
      "[0.75862069]\n",
      "[0.57471264]\n",
      "[0.59770115]\n",
      "[0.6091954]\n",
      "[0.62068966]\n",
      "[0.86206897]\n",
      "[0.11494253]\n",
      "[0.01149425]\n",
      "[0.51724138]\n",
      "[0.71264368]\n",
      "[0.25287356]\n",
      "[0.10344828]\n",
      "[0.47126437]\n",
      "[0.1954023]\n",
      "[0.59770115]\n",
      "[0.87356322]\n",
      "[0.20689655]\n",
      "[0.89655172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18390805]\n",
      "[0.14942529]\n",
      "[0.74712644]\n",
      "[0.73563218]\n",
      "[0.04597701]\n",
      "[0.12643678]\n",
      "[0.34482759]\n",
      "[0.90804598]\n",
      "[0.03448276]\n",
      "[0.14942529]\n",
      "[0.1954023]\n",
      "[0.12643678]\n",
      "[0.11494253]\n",
      "[0.06896552]\n",
      "[0.37931034]\n",
      "[1.]\n",
      "[0.17241379]\n",
      "[0.75862069]\n",
      "[0.55172414]\n",
      "[0.90804598]\n",
      "[0.6091954]\n",
      "[0.03448276]\n",
      "[0.63218391]\n",
      "[0.85057471]\n",
      "[0.54022989]\n",
      "[0.45977011]\n",
      "[0.89655172]\n",
      "[0.91954023]\n",
      "[0.50574713]\n",
      "[0.68965517]\n",
      "[0.20689655]\n",
      "[0.]\n",
      "[0.59770115]\n",
      "[0.17241379]\n",
      "[0.71264368]\n",
      "[0.94252874]\n",
      "[0.96551724]\n",
      "[0.49425287]\n",
      "[0.57471264]\n",
      "[0.40229885]\n",
      "[0.87356322]\n",
      "[0.79310345]\n",
      "[0.08045977]\n",
      "[0.83908046]\n",
      "[0.85057471]\n",
      "[0.7816092]\n",
      "[0.65517241]\n",
      "[0.89655172]\n",
      "[0.43678161]\n",
      "[0.54022989]\n",
      "[0.28735632]\n",
      "[0.65517241]\n",
      "[0.52873563]\n",
      "[0.85057471]\n",
      "[0.]\n",
      "[0.12643678]\n",
      "[0.68965517]\n",
      "[0.11494253]\n",
      "[0.2183908]\n",
      "[0.44827586]\n",
      "[0.7816092]\n",
      "[0.06896552]\n",
      "[0.81609195]\n",
      "[0.94252874]\n",
      "[0.03448276]\n",
      "[0.09195402]\n",
      "[0.66666667]\n",
      "[0.70114943]\n",
      "[0.70114943]\n",
      "[0.81609195]\n",
      "[0.51724138]\n",
      "[0.35632184]\n",
      "[0.90804598]\n",
      "[1.]\n",
      "[0.64367816]\n",
      "[0.02298851]\n",
      "[0.1954023]\n",
      "[0.01149425]\n",
      "[0.52873563]\n",
      "[0.51724138]\n",
      "[0.79310345]\n",
      "[0.83908046]\n",
      "[1.]\n",
      "[0.47126437]\n",
      "[0.1954023]\n",
      "[0.73563218]\n",
      "[0.63218391]\n",
      "[0.87356322]\n",
      "[0.13793103]\n",
      "[0.18390805]\n",
      "[0.85057471]\n",
      "[0.95402299]\n",
      "[0.85057471]\n",
      "[0.51724138]\n",
      "[0.31034483]\n",
      "[0.98850575]\n",
      "[0.89655172]\n",
      "[0.09195402]\n",
      "[0.6091954]\n",
      "[0.59770115]\n",
      "[0.50574713]\n",
      "[0.2183908]\n",
      "[0.83908046]\n",
      "[0.93103448]\n",
      "[0.56321839]\n",
      "[0.73563218]\n",
      "[0.03448276]\n",
      "[0.08045977]\n",
      "[0.65517241]\n",
      "[0.47126437]\n",
      "[0.29885057]\n",
      "[0.59770115]\n",
      "[0.64367816]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "for i in scaled_train_samples:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b4b185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a88f9",
   "metadata": {},
   "source": [
    "At this point, we've generated some sample raw data, put it into the numpy format that our model will require, and rescaled it to a scale ranging from 0 to 1.\n",
    "\n",
    "In an upcoming episode, we'll use this data to train a neural network and see what kind of results we can get. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80264d7",
   "metadata": {},
   "source": [
    "## Create an artificial neural network with TensorFlow's Keras API\n",
    "\n",
    "In this episode, we'll demonstrate how to create a simple artificial neural network using a **Sequential model** from the Keras API integrated within TensorFlow.\n",
    "\n",
    "https://deeplizard.com/images/png/deep%20neural%20network%20with%204%20layers.png\n",
    "\n",
    "In the last episode, we generated some data from an imagined clinical trial, and now we'll build a simple model for which we can train on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0283b1",
   "metadata": {},
   "source": [
    "## Code Setup\n",
    "\n",
    "First, we need to import all the libraries we'll be making use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b132700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b4af1",
   "metadata": {},
   "source": [
    "We'll use all of these modules, except for the last two, to **build our neural network**. Note that we'll make use of the last two modules in the next episode when we **train** the model.\n",
    "\n",
    "A GPU is not required to follow this course, but if you are using one, you'll need to first follow the GPU setup we covered in a previous episode. We can then check to be sure that TensorFlow is able to identify the GPU using the code below. It's also useful to enable memory growth on the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d790f6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208acd8",
   "metadata": {},
   "source": [
    "##  Build a Sequential Model\n",
    "\n",
    "Let's now create our model. We first create a variable named model and define it as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6275178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e1dc2",
   "metadata": {},
   "source": [
    "***model*** is an instance of a Sequential object. A tf.keras.Sequential model is a linear stack of layers. It accepts a list, and each element in the list should be a layer.\n",
    "\n",
    "As you can see, we have passed a list of layers to the Sequential constructor. Let's go through each of the layers in this list now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8818e",
   "metadata": {},
   "source": [
    "### First Hidden Layer\n",
    "\n",
    "Our first layer is a **Dense** layer. This type of layer is our standard **fully-connected or densely-connected** neural network layer. The first required parameter that the Dense layer expects is the number of neurons or units the layer has, and we're arbitrarily setting this to 16.\n",
    "\n",
    "Additionally, the model needs to know the shape of the input data. For this reason, we specify the shape of the input data in the first hidden layer in the model (and only this layer). The parameter called input_shape is how we specify this.\n",
    "\n",
    "As discussed, we'll be training our network on the data that we generated and processed in the previous episode, and recall, this data is one-dimensional. The input_shape parameter expects a tuple of integers that matches the shape of the input data, so we correspondingly specify (1,) as the input_shape of our one-dimensional data.\n",
    "\n",
    "You can think of the way we specify the input_shape here as acting as an implicit input layer. The input layer of a neural network is the underlying raw data itself, therefore we don't create an explicit input layer. This first Dense layer that we're working with now is actually the first hidden layer.\n",
    "\n",
    "Lastly, an optional parameter that we'll set for the Dense layer is the activation function to use after this layer. We'll use the popular choice of **relu**. Note, if you don't explicitly set an activation function, then Keras will use the linear activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72410f",
   "metadata": {},
   "source": [
    "### Second Hidden Layer\n",
    "\n",
    "Our next layer will also be a Dense layer, and this one will have 32 nodes. The choice of how many neurons this node has is also arbitrary, as the idea is to create a simple model, and then test and experiment with it. If we notice that it is insufficient, then at that time, we can troubleshoot the issue and begin experimenting with changing parameters, like number of layers, nodes, etc.\n",
    "\n",
    "This Dense layer will also use relu as its activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54410d49",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "\n",
    "Lastly, we specify the output layer. This layer is also a Dense layer, and it will have **2 neurons**. This is because we have two possible outputs: either a patient experienced side effects, or the patient did not experience side effects.\n",
    "\n",
    "This time, the activation function we'll use is softmax, which will give us a probability distribution among the possible outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587f781",
   "metadata": {},
   "source": [
    "Note that we can call summary() on our model to get a quick visualization of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc1fda17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69130b",
   "metadata": {},
   "source": [
    "Now we've created our very first model using the intuitive tf.keras.Sequential model type. In the next episode we'll train this model on the data we created last time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7268eb5",
   "metadata": {},
   "source": [
    "## Train an Artificial Neural Network with TensorFlow's Keras API\n",
    "\n",
    "In this episode, we'll demonstrate how to train an artificial neural network using the Keras API integrated within TensorFlow.\n",
    "\n",
    "In the previous episode, we went through the steps to build a simple network, and now we'll focus on training it using data we generated in an even earlier episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee974ce",
   "metadata": {},
   "source": [
    "##  Compiling the model\n",
    "\n",
    "The first thing we need to do to get the model ready for training is call the **compile()** function on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d1852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74822d4",
   "metadata": {},
   "source": [
    "This function configures the model for training and expects a number of parameters. First, we specify the **optimizer Adam**. Adam accepts an optional parameter **learning_rate**, which we'll set to 0.0001. Adam optimization is a **stochastic gradient descent (SGD) method**.\n",
    "\n",
    "The next parameter we specify is **loss**. We'll be using **sparse_categorical_crossentropy**, given that our labels are in integer format.\n",
    "\n",
    "Note that when we have only two classes, we could instead configure our output layer to have only one output, rather than two, and use **binary_crossentropy as our loss**, rather than categorical_crossentropy. Both options work equally well and achieve the exact same result.\n",
    "\n",
    "With **binary_crossentropy**, however, the last layer would need to use **sigmoid**, rather than softmax, as its activation function.\n",
    "\n",
    "Moving on, the last parameter we specify in **compile()** is metrics. This parameter expects a list of metrics that we'd like to be evaluated by the model during training and testing. We'll set this to a list that contains the string **âaccuracy'**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece981c",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    " Now that the model is compiled, we can train it using the fit() function.\n",
    " \n",
    "The first item that we pass in to the fit() function is the training set **x**. Recall from a previous episode, we created the training set and gave it the name scaled_train_samples.\n",
    "\n",
    "The next parameter that we set is the labels for the training set **y**, which we previously gave the name train_labels. We then specify the **batch_size**.\n",
    "\n",
    "Next, we specify how many **epochs** we want to run. We set this to 30. Note that an epoch is a single pass of all the data to the network.\n",
    "\n",
    "Lastly, we specify **verbose=2**. This just specifies how much output to the console we want to see during each epoch of training. The verbosity levels range from 0 to 2, so we're getting the most verbose output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604df616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae722e8",
   "metadata": {},
   "source": [
    "We can see corresponding output for each of the 30 epochs. Judging by the loss and accuracy, we can see that both metrics steadily improve over time with accuracy reaching almost 94% and loss steadily decreasing until we reach 0.25.\n",
    "\n",
    "Note that although this is a very simple model trained on simple data, without much effort, we were able to reach pretty good results in a relatively quick manner of time. In subsequent episodes, we'll demo more complex models as well as more complex data, but hopefully you've become encouraged by how easily we were able to get started with tf.keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8245d26",
   "metadata": {},
   "source": [
    "##  Build a validation set with TensorFlow's Keras API\n",
    "\n",
    "In this episode, we'll demonstrate how to use TensorFlow's Keras API to create a validation set on-the-fly during training.\n",
    "\n",
    "We'll continue working with the same model we built and trained in the previous episode, but first, let's discuss what exactly a validation set is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a734b09",
   "metadata": {},
   "source": [
    "## What is a validation set?\n",
    "\n",
    "Recall that we previously built a training set on which we trained our model. With each epoch that our model is trained, the model will continue to learn the features and characteristics of the data in this training set.\n",
    "\n",
    "The hope is that later we can take this model, apply it to new data, and have the model accurately predict on data that it hasn't seen before based solely on what it learned from the training set.\n",
    "\n",
    "Now, let's discuss where the addition of a validation set comes into play.\n",
    "\n",
    "Before training begins, we can choose to remove a portion of the training set and place it in a validation set. Then, during training, the model will train only on the training set, and it will validate by evaluating the data in the validation set.\n",
    "\n",
    "Essentially, the model is learning the features of the data in the training set, taking what it's learned from this data, and then predicting on the validation set. During each epoch, we will see not only the loss and accuracy results for the training set, but also for the validation set.\n",
    "\n",
    "This allows us to see how well the model is generalizing on data it wasn't trained on because, recall, the validation data should not be part of the training data.\n",
    "\n",
    "This also helps us see whether or not the model is **overfitting**. Overfitting occurs when the model only learns the specifics of the training data and is unable to generalize well on data that it wasn't trained on. Now let's discuss how we can create a validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de425a6",
   "metadata": {},
   "source": [
    "## Creating A Validation Set\n",
    "\n",
    "There are two ways to create a validation set to use with a **tf.keras.Sequential** model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19838f16",
   "metadata": {},
   "source": [
    "###  Manually create validation set\n",
    "\n",
    "The first way is to create a data structure to hold a validation set, and place data directly in that structure in the same nature we did for the training set.\n",
    "\n",
    "This data structure should be a tuple **valid_set = (x_val, y_val)** of Numpy arrays or tensors, where x_val is a numpy array or tensor containing validation samples, and y_val is a numpy array or tensor containing validation labels.\n",
    "\n",
    "When we call **model.fit()**, we would pass in the validation set in addition to the training set. We pass the validation set by specifying the **validation_data** parameter.\n",
    "\n",
    "model.fit(\n",
    "      x=scaled_train_samples\n",
    "    , y=train_labels\n",
    "    , validation_data=valid_set\n",
    "    , batch_size=10\n",
    "    , epochs=30\n",
    "    , verbose=2\n",
    ")\n",
    "\n",
    "When the model trains, it would continue to train only on the training set, but additionally, it would also be evaluating the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ab8f1",
   "metadata": {},
   "source": [
    "###  Create Validation Set With Keras\n",
    "\n",
    "There is another way to create a validation set, and it saves a step!\n",
    "\n",
    "If we don't already have a specified validation set created, then when we call **model.fit()**, we can set a value for the **validation_split** parameter. It expects a fractional number between 0 and 1. Suppose that we set this parameter to 0.1.\n",
    "\n",
    "model.fit(\n",
    "      x=scaled_train_samples\n",
    "    , y=train_labels\n",
    "    , validation_split=0.1\n",
    "    , batch_size=10\n",
    "    , epochs=30\n",
    "    , verbose=2\n",
    ")\n",
    "\n",
    "With this parameter specified, Keras will split apart a fraction (10% in this example) of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "\n",
    "Note that the **fit()** function shuffles the data before each epoch by default. When specifying the **validation_split** parameter, however, the validation data is selected from the last samples in the x and y data before shuffling.\n",
    "\n",
    "**Therefore, in the case we're using validation_split in this way to create our validation data, we need to be sure that our data has been shuffled ahead of time, like we previously did in an earlier episode**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06ef10",
   "metadata": {},
   "source": [
    "##  Interpret Validation Metrics\n",
    "\n",
    "Now, regardless of which method we use to create validation data, when we call **model.fit()**, then in addition to loss and accuracy being displayed for each epoch as we saw last time, we will now also see **val_loss** and **val_acc** to track the loss and accuracy on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de58256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "189/189 - 0s - loss: 0.6772 - accuracy: 0.5111 - val_loss: 0.6599 - val_accuracy: 0.5857\n",
      "Epoch 2/30\n",
      "189/189 - 0s - loss: 0.6441 - accuracy: 0.6212 - val_loss: 0.6231 - val_accuracy: 0.7095\n",
      "Epoch 3/30\n",
      "189/189 - 0s - loss: 0.6043 - accuracy: 0.7344 - val_loss: 0.5772 - val_accuracy: 0.7905\n",
      "Epoch 4/30\n",
      "189/189 - 0s - loss: 0.5644 - accuracy: 0.7783 - val_loss: 0.5395 - val_accuracy: 0.8238\n",
      "Epoch 5/30\n",
      "189/189 - 0s - loss: 0.5288 - accuracy: 0.8196 - val_loss: 0.5012 - val_accuracy: 0.8333\n",
      "Epoch 6/30\n",
      "189/189 - 0s - loss: 0.4943 - accuracy: 0.8444 - val_loss: 0.4677 - val_accuracy: 0.8476\n",
      "Epoch 7/30\n",
      "189/189 - 0s - loss: 0.4638 - accuracy: 0.8614 - val_loss: 0.4368 - val_accuracy: 0.8571\n",
      "Epoch 8/30\n",
      "189/189 - 0s - loss: 0.4364 - accuracy: 0.8741 - val_loss: 0.4094 - val_accuracy: 0.8714\n",
      "Epoch 9/30\n",
      "189/189 - 0s - loss: 0.4118 - accuracy: 0.8820 - val_loss: 0.3855 - val_accuracy: 0.8810\n",
      "Epoch 10/30\n",
      "189/189 - 0s - loss: 0.3901 - accuracy: 0.8884 - val_loss: 0.3639 - val_accuracy: 0.8905\n",
      "Epoch 11/30\n",
      "189/189 - 0s - loss: 0.3713 - accuracy: 0.8921 - val_loss: 0.3458 - val_accuracy: 0.9143\n",
      "Epoch 12/30\n",
      "189/189 - 0s - loss: 0.3551 - accuracy: 0.8963 - val_loss: 0.3298 - val_accuracy: 0.9143\n",
      "Epoch 13/30\n",
      "189/189 - 0s - loss: 0.3415 - accuracy: 0.9016 - val_loss: 0.3166 - val_accuracy: 0.9333\n",
      "Epoch 14/30\n",
      "189/189 - 0s - loss: 0.3297 - accuracy: 0.9095 - val_loss: 0.3048 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "189/189 - 0s - loss: 0.3199 - accuracy: 0.9079 - val_loss: 0.2955 - val_accuracy: 0.9333\n",
      "Epoch 16/30\n",
      "189/189 - 0s - loss: 0.3118 - accuracy: 0.9132 - val_loss: 0.2871 - val_accuracy: 0.9476\n",
      "Epoch 17/30\n",
      "189/189 - 0s - loss: 0.3048 - accuracy: 0.9138 - val_loss: 0.2801 - val_accuracy: 0.9476\n",
      "Epoch 18/30\n",
      "189/189 - 0s - loss: 0.2988 - accuracy: 0.9153 - val_loss: 0.2741 - val_accuracy: 0.9476\n",
      "Epoch 19/30\n",
      "189/189 - 0s - loss: 0.2940 - accuracy: 0.9169 - val_loss: 0.2690 - val_accuracy: 0.9476\n",
      "Epoch 20/30\n",
      "189/189 - 0s - loss: 0.2899 - accuracy: 0.9190 - val_loss: 0.2648 - val_accuracy: 0.9524\n",
      "Epoch 21/30\n",
      "189/189 - 0s - loss: 0.2861 - accuracy: 0.9217 - val_loss: 0.2606 - val_accuracy: 0.9524\n",
      "Epoch 22/30\n",
      "189/189 - 0s - loss: 0.2831 - accuracy: 0.9217 - val_loss: 0.2573 - val_accuracy: 0.9524\n",
      "Epoch 23/30\n",
      "189/189 - 0s - loss: 0.2805 - accuracy: 0.9217 - val_loss: 0.2547 - val_accuracy: 0.9524\n",
      "Epoch 24/30\n",
      "189/189 - 0s - loss: 0.2783 - accuracy: 0.9228 - val_loss: 0.2525 - val_accuracy: 0.9571\n",
      "Epoch 25/30\n",
      "189/189 - 0s - loss: 0.2762 - accuracy: 0.9228 - val_loss: 0.2499 - val_accuracy: 0.9571\n",
      "Epoch 26/30\n",
      "189/189 - 0s - loss: 0.2744 - accuracy: 0.9217 - val_loss: 0.2479 - val_accuracy: 0.9571\n",
      "Epoch 27/30\n",
      "189/189 - 0s - loss: 0.2728 - accuracy: 0.9270 - val_loss: 0.2457 - val_accuracy: 0.9571\n",
      "Epoch 28/30\n",
      "189/189 - 0s - loss: 0.2717 - accuracy: 0.9254 - val_loss: 0.2439 - val_accuracy: 0.9524\n",
      "Epoch 29/30\n",
      "189/189 - 0s - loss: 0.2702 - accuracy: 0.9270 - val_loss: 0.2422 - val_accuracy: 0.9524\n",
      "Epoch 30/30\n",
      "189/189 - 0s - loss: 0.2693 - accuracy: 0.9233 - val_loss: 0.2412 - val_accuracy: 0.9571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16eb79bdf40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_train_samples,\n",
    "          y=train_labels,\n",
    "          validation_split=0.1,\n",
    "          batch_size=10,\n",
    "          epochs=30,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792666a",
   "metadata": {},
   "source": [
    "We can now see not only how well our model is learning the features of the training data, but also how well the model is generalizing to new, unseen data from the validation set. Next, we'll see how to use our model for inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257ba1f",
   "metadata": {},
   "source": [
    "## Neural Network Predictions with TensorFlow's Keras API\n",
    "\n",
    "In this episode, we'll demonstrate how to use a neural network for **inference** to make predictions on data from a test set. We'll continue working with the same **tf.keras.Sequential** model and data that we've used in the last few episodes to do so.\n",
    "\n",
    "As we touched on previously, when we train a model, the hope is that we'll later be able to take the trained model, apply it to new data, and have the model generalize and accurately predict on data it hasn't seen before.\n",
    "\n",
    "For example, suppose we have a model that categorizes images of cats or dogs and that the training data contained thousands of images of cats and dogs from a particular data set online. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21756f61",
   "metadata": {},
   "source": [
    "###  What is Inference?\n",
    "\n",
    "Now suppose that later we want to take this model and use it to predict on other images of cats and dogs from a different data set. The hope is that, even though our model wasn't exposed to these particular dog and cat images during training, it will still be able to accurately make predictions for them based on what it's learned from the cat and dog data set from which it was trained.\n",
    "\n",
    "We call this process **inference**, as the model is using its knowledge gained from training and using it to infer a prediction or result.\n",
    "\n",
    "At this point, the model we've been working with over the past few episodes has now been trained and validated. Given the results we've seen from the validation data, it appears that this model should do well on predicting on a new test set.\n",
    "\n",
    "**Note that the test set is the set of data used specifically for inference after training has concluded**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3f2d7",
   "metadata": {},
   "source": [
    "## Creating The Test Set\n",
    "\n",
    "We'll create a test set in the same fashion for which we created the training set. In general, the test set should always be processed in the same way as the training set.\n",
    "\n",
    "We won't go step-by-step over the code that generates and processes the test data below, as it has already been covered in detail in an earlier episode where we generated the training data, but be sure you have all the imports in place from the previous episodes, as well as all of the existing code up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce1344da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels =  []\n",
    "test_samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "\n",
    "    # The 5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    # The 95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "\n",
    "    # The 95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)\n",
    "\n",
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1bd0e",
   "metadata": {},
   "source": [
    "Note that the **MinMaxScaler object scaler** we're making use of at the end of this code block was defined in a previous episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6ea5b",
   "metadata": {},
   "source": [
    "##  Evaluating the test set\n",
    "\n",
    "To get predictions from the model for the test set, we call **model.predict()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e3ceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(\n",
    "      x=scaled_test_samples\n",
    "    , batch_size=10\n",
    "    , verbose=0\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dda91",
   "metadata": {},
   "source": [
    "To this function, we pass in the **test samples x**, specify a **batch_size**, and specify which level of verbosity we want from log messages during prediction generation. The output from the predictions won't be relevant for us, so we're setting verbose=0 for no output.\n",
    "\n",
    "**Note that, unlike with training and validation sets, we do not pass the labels of the test set to the model during the inference stage.**\n",
    "\n",
    "To see what the model's predictions look like, we can iterate over them and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0003ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22598827 0.7740117 ]\n",
      "[0.9305548  0.06944521]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.07287429 0.92712575]\n",
      "[0.9305548  0.06944521]\n",
      "[0.07723728 0.92276275]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.9715029  0.02849705]\n",
      "[0.970064   0.02993602]\n",
      "[0.51238465 0.48761535]\n",
      "[0.87766784 0.12233215]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.06112138 0.9388786 ]\n",
      "[0.97103095 0.02896909]\n",
      "[0.06112138 0.9388786 ]\n",
      "[0.970064   0.02993602]\n",
      "[0.0648227 0.9351773]\n",
      "[0.24917096 0.7508291 ]\n",
      "[0.9711891  0.02881091]\n",
      "[0.970064   0.02993602]\n",
      "[0.04287384 0.9571262 ]\n",
      "[0.41711038 0.5828896 ]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.04550481 0.95449525]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.8632398  0.13676022]\n",
      "[0.97103095 0.02896909]\n",
      "[0.9705513  0.02944872]\n",
      "[0.94270355 0.05729641]\n",
      "[0.16585408 0.8341459 ]\n",
      "[0.79087406 0.20912594]\n",
      "[0.0648227 0.9351773]\n",
      "[0.970064   0.02993602]\n",
      "[0.8474058 0.1525942]\n",
      "[0.9722731  0.02772687]\n",
      "[0.3276479  0.67235214]\n",
      "[0.8907664  0.10923357]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.02814519 0.97185487]\n",
      "[0.96577847 0.0342215 ]\n",
      "[0.0317618  0.96823823]\n",
      "[0.10644677 0.89355326]\n",
      "[0.7453704 0.2546296]\n",
      "[0.07723728 0.92276275]\n",
      "[0.8112675  0.18873248]\n",
      "[0.06112138 0.9388786 ]\n",
      "[0.08701959 0.9129804 ]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.06873938 0.93126065]\n",
      "[0.3276479  0.67235214]\n",
      "[0.14888601 0.85111403]\n",
      "[0.03582601 0.96417403]\n",
      "[0.5758304 0.4241696]\n",
      "[0.08701959 0.9129804 ]\n",
      "[0.0317618  0.96823823]\n",
      "[0.9698998 0.0301002]\n",
      "[0.14888601 0.85111403]\n",
      "[0.9705513  0.02944872]\n",
      "[0.51238465 0.48761535]\n",
      "[0.9560004  0.04399956]\n",
      "[0.97210807 0.02789202]\n",
      "[0.63687134 0.3631287 ]\n",
      "[0.97165865 0.02834138]\n",
      "[0.07287429 0.92712575]\n",
      "[0.18433718 0.81566286]\n",
      "[0.06112138 0.9388786 ]\n",
      "[0.48038107 0.519619  ]\n",
      "[0.0317618  0.96823823]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.63687134 0.3631287 ]\n",
      "[0.9711891  0.02881091]\n",
      "[0.9703898  0.02961029]\n",
      "[0.06112138 0.9388786 ]\n",
      "[0.90261805 0.09738193]\n",
      "[0.90261805 0.09738193]\n",
      "[0.8474058 0.1525942]\n",
      "[0.63687134 0.3631287 ]\n",
      "[0.92274994 0.07725005]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.18433718 0.81566286]\n",
      "[0.90261805 0.09738193]\n",
      "[0.66593915 0.3340608 ]\n",
      "[0.5758304 0.4241696]\n",
      "[0.97196746 0.02803249]\n",
      "[0.14888601 0.85111403]\n",
      "[0.9476931  0.05230685]\n",
      "[0.93715864 0.06284136]\n",
      "[0.22598827 0.7740117 ]\n",
      "[0.13337627 0.8666237 ]\n",
      "[0.07723728 0.92276275]\n",
      "[0.9722731  0.02772687]\n",
      "[0.913309   0.08669098]\n",
      "[0.97210807 0.02789202]\n",
      "[0.97196746 0.02803249]\n",
      "[0.3863426  0.61365736]\n",
      "[0.30008185 0.69991815]\n",
      "[0.07287429 0.92712575]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.48038107 0.519619  ]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.18433718 0.81566286]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.08183844 0.9181616 ]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.8474058 0.1525942]\n",
      "[0.913309   0.08669098]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.06873938 0.93126065]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.07287429 0.92712575]\n",
      "[0.11925583 0.88074416]\n",
      "[0.97212076 0.02787927]\n",
      "[0.16585408 0.8341459 ]\n",
      "[0.8300995 0.1699005]\n",
      "[0.5442872 0.4557128]\n",
      "[0.48038107 0.519619  ]\n",
      "[0.08701959 0.9129804 ]\n",
      "[0.9703898  0.02961029]\n",
      "[0.27389032 0.7261097 ]\n",
      "[0.11925583 0.88074416]\n",
      "[0.06112138 0.9388786 ]\n",
      "[0.97212076 0.02787927]\n",
      "[0.7453704 0.2546296]\n",
      "[0.02814519 0.97185487]\n",
      "[0.04550481 0.95449525]\n",
      "[0.9560004  0.04399956]\n",
      "[0.7203112 0.2796887]\n",
      "[0.35645655 0.6435434 ]\n",
      "[0.9560004  0.04399956]\n",
      "[0.0648227 0.9351773]\n",
      "[0.97181356 0.02818651]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.51238465 0.48761535]\n",
      "[0.04550481 0.95449525]\n",
      "[0.0317618  0.96823823]\n",
      "[0.7203112 0.2796887]\n",
      "[0.9717914  0.02820868]\n",
      "[0.02814519 0.97185487]\n",
      "[0.9698998 0.0301002]\n",
      "[0.03373489 0.9662651 ]\n",
      "[0.9629288  0.03707123]\n",
      "[0.96577847 0.0342215 ]\n",
      "[0.94270355 0.05729641]\n",
      "[0.3863426  0.61365736]\n",
      "[0.8632398  0.13676022]\n",
      "[0.06873938 0.93126065]\n",
      "[0.13337627 0.8666237 ]\n",
      "[0.9708719  0.02912812]\n",
      "[0.24917096 0.7508291 ]\n",
      "[0.8632398  0.13676022]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.27389032 0.7261097 ]\n",
      "[0.970064   0.02993602]\n",
      "[0.03582601 0.96417403]\n",
      "[0.04287384 0.9571262 ]\n",
      "[0.9711891  0.02881091]\n",
      "[0.913309   0.08669098]\n",
      "[0.9682008  0.03179925]\n",
      "[0.02814519 0.97185487]\n",
      "[0.9703898  0.02961029]\n",
      "[0.8907664  0.10923357]\n",
      "[0.05764208 0.9423579 ]\n",
      "[0.4485375  0.55146253]\n",
      "[0.9682008  0.03179925]\n",
      "[0.48038107 0.519619  ]\n",
      "[0.5758304 0.4241696]\n",
      "[0.97103095 0.02896909]\n",
      "[0.97022724 0.02977272]\n",
      "[0.9682008  0.03179925]\n",
      "[0.913309   0.08669098]\n",
      "[0.02814519 0.97185487]\n",
      "[0.51238465 0.48761535]\n",
      "[0.9708719  0.02912812]\n",
      "[0.04287384 0.9571262 ]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.97022724 0.02977272]\n",
      "[0.18433718 0.81566286]\n",
      "[0.22598827 0.7740117 ]\n",
      "[0.6937991  0.30620095]\n",
      "[0.0317618  0.96823823]\n",
      "[0.9520172  0.04798277]\n",
      "[0.9705513  0.02944872]\n",
      "[0.41711038 0.5828896 ]\n",
      "[0.3863426  0.61365736]\n",
      "[0.9520172  0.04798277]\n",
      "[0.05434936 0.9456507 ]\n",
      "[0.9560004  0.04399956]\n",
      "[0.6937991  0.30620095]\n",
      "[0.97134644 0.02865357]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.3863426  0.61365736]\n",
      "[0.3276479  0.67235214]\n",
      "[0.90261805 0.09738193]\n",
      "[0.60676754 0.39323246]\n",
      "[0.24917096 0.7508291 ]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.08183844 0.9181616 ]\n",
      "[0.63687134 0.3631287 ]\n",
      "[0.5758304 0.4241696]\n",
      "[0.06873938 0.93126065]\n",
      "[0.9305548  0.06944521]\n",
      "[0.13337627 0.8666237 ]\n",
      "[0.0648227 0.9351773]\n",
      "[0.97165865 0.02834138]\n",
      "[0.8112675  0.18873248]\n",
      "[0.05123449 0.9487655 ]\n",
      "[0.87766784 0.12233215]\n",
      "[0.04550481 0.95449525]\n",
      "[0.03582601 0.96417403]\n",
      "[0.97134644 0.02865357]\n",
      "[0.03582601 0.96417403]\n",
      "[0.7203112 0.2796887]\n",
      "[0.05434936 0.9456507 ]\n",
      "[0.9715029  0.02849705]\n",
      "[0.22598827 0.7740117 ]\n",
      "[0.05434936 0.9456507 ]\n",
      "[0.97103095 0.02896909]\n",
      "[0.8300995 0.1699005]\n",
      "[0.8112675  0.18873248]\n",
      "[0.05123449 0.9487655 ]\n",
      "[0.48038107 0.519619  ]\n",
      "[0.60676754 0.39323246]\n",
      "[0.03373489 0.9662651 ]\n",
      "[0.8632398  0.13676022]\n",
      "[0.14888601 0.85111403]\n",
      "[0.48038107 0.519619  ]\n",
      "[0.35645655 0.6435434 ]\n",
      "[0.05764208 0.9423579 ]\n",
      "[0.93715864 0.06284136]\n",
      "[0.97210807 0.02789202]\n",
      "[0.11925583 0.88074416]\n",
      "[0.8632398  0.13676022]\n",
      "[0.27389032 0.7261097 ]\n",
      "[0.04287384 0.9571262 ]\n",
      "[0.10644677 0.89355326]\n",
      "[0.4485375  0.55146253]\n",
      "[0.27389032 0.7261097 ]\n",
      "[0.79087406 0.20912594]\n",
      "[0.97210807 0.02789202]\n",
      "[0.66593915 0.3340608 ]\n",
      "[0.9476931  0.05230685]\n",
      "[0.5758304 0.4241696]\n",
      "[0.24917096 0.7508291 ]\n",
      "[0.03582601 0.96417403]\n",
      "[0.97196746 0.02803249]\n",
      "[0.63687134 0.3631287 ]\n",
      "[0.02814519 0.97185487]\n",
      "[0.03373489 0.9662651 ]\n",
      "[0.9705513  0.02944872]\n",
      "[0.08701959 0.9129804 ]\n",
      "[0.9708719  0.02912812]\n",
      "[0.9520172  0.04798277]\n",
      "[0.03582601 0.96417403]\n",
      "[0.97196746 0.02803249]\n",
      "[0.22598827 0.7740117 ]\n",
      "[0.9715029  0.02849705]\n",
      "[0.3863426  0.61365736]\n",
      "[0.9717914  0.02820868]\n",
      "[0.913309   0.08669098]\n",
      "[0.03373489 0.9662651 ]\n",
      "[0.9705513  0.02944872]\n",
      "[0.06873938 0.93126065]\n",
      "[0.05123449 0.9487655 ]\n",
      "[0.0317618  0.96823823]\n",
      "[0.970712   0.02928799]\n",
      "[0.9629288  0.03707123]\n",
      "[0.07723728 0.92276275]\n",
      "[0.03373489 0.9662651 ]\n",
      "[0.9708719  0.02912812]\n",
      "[0.30008185 0.69991815]\n",
      "[0.3276479  0.67235214]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.9560004  0.04399956]\n",
      "[0.02814519 0.97185487]\n",
      "[0.92274994 0.07725005]\n",
      "[0.97212076 0.02787927]\n",
      "[0.8907664  0.10923357]\n",
      "[0.02814519 0.97185487]\n",
      "[0.970712   0.02928799]\n",
      "[0.30008185 0.69991815]\n",
      "[0.08183844 0.9181616 ]\n",
      "[0.97165865 0.02834138]\n",
      "[0.9682008  0.03179925]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.7689047  0.23109533]\n",
      "[0.9476931  0.05230685]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.66593915 0.3340608 ]\n",
      "[0.970712   0.02928799]\n",
      "[0.97196746 0.02803249]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.30008185 0.69991815]\n",
      "[0.93715864 0.06284136]\n",
      "[0.07723728 0.92276275]\n",
      "[0.0317618  0.96823823]\n",
      "[0.05434936 0.9456507 ]\n",
      "[0.41711038 0.5828896 ]\n",
      "[0.97212076 0.02787927]\n",
      "[0.22598827 0.7740117 ]\n",
      "[0.8112675  0.18873248]\n",
      "[0.41711038 0.5828896 ]\n",
      "[0.9715029  0.02849705]\n",
      "[0.60676754 0.39323246]\n",
      "[0.9717914  0.02820868]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.04038858 0.9596114 ]\n",
      "[0.0317618  0.96823823]\n",
      "[0.03373489 0.9662651 ]\n",
      "[0.9305548  0.06944521]\n",
      "[0.8112675  0.18873248]\n",
      "[0.04550481 0.95449525]\n",
      "[0.30008185 0.69991815]\n",
      "[0.7453704 0.2546296]\n",
      "[0.959667 0.040333]\n",
      "[0.97242117 0.0275788 ]\n",
      "[0.02990055 0.97009945]\n",
      "[0.30008185 0.69991815]\n",
      "[0.48038107 0.519619  ]\n",
      "[0.97210807 0.02789202]\n",
      "[0.93715864 0.06284136]\n",
      "[0.04038858 0.9596114 ]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.04550481 0.95449525]\n",
      "[0.05123449 0.9487655 ]\n",
      "[0.7453704 0.2546296]\n",
      "[0.04038858 0.9596114 ]\n",
      "[0.970712   0.02928799]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.9705513  0.02944872]\n",
      "[0.90261805 0.09738193]\n",
      "[0.10644677 0.89355326]\n",
      "[0.97022724 0.02977272]\n",
      "[0.30008185 0.69991815]\n",
      "[0.06873938 0.93126065]\n",
      "[0.13337627 0.8666237 ]\n",
      "[0.66593915 0.3340608 ]\n",
      "[0.9705513  0.02944872]\n",
      "[0.07287429 0.92712575]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.9476931  0.05230685]\n",
      "[0.0317618  0.96823823]\n",
      "[0.3863426  0.61365736]\n",
      "[0.18433718 0.81566286]\n",
      "[0.9560004  0.04399956]\n",
      "[0.03582601 0.96417403]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.09499053 0.9050095 ]\n",
      "[0.30008185 0.69991815]\n",
      "[0.9717914  0.02820868]\n",
      "[0.9560004  0.04399956]\n",
      "[0.8300995 0.1699005]\n",
      "[0.03582601 0.96417403]\n",
      "[0.3276479  0.67235214]\n",
      "[0.05764208 0.9423579 ]\n",
      "[0.9702948  0.02970518]\n",
      "[0.3863426  0.61365736]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.08701959 0.9129804 ]\n",
      "[0.08701959 0.9129804 ]\n",
      "[0.18433718 0.81566286]\n",
      "[0.04550481 0.95449525]\n",
      "[0.18433718 0.81566286]\n",
      "[0.970712   0.02928799]\n",
      "[0.97210807 0.02789202]\n",
      "[0.22598827 0.7740117 ]\n",
      "[0.05434936 0.9456507 ]\n",
      "[0.14888601 0.85111403]\n",
      "[0.06873938 0.93126065]\n",
      "[0.97103095 0.02896909]\n",
      "[0.97181356 0.02818651]\n",
      "[0.35645655 0.6435434 ]\n",
      "[0.90261805 0.09738193]\n",
      "[0.02814519 0.97185487]\n",
      "[0.3276479  0.67235214]\n",
      "[0.04287384 0.9571262 ]\n",
      "[0.8300995 0.1699005]\n",
      "[0.93715864 0.06284136]\n",
      "[0.11925583 0.88074416]\n",
      "[0.13337627 0.8666237 ]\n",
      "[0.02990055 0.97009945]\n",
      "[0.35645655 0.6435434 ]\n",
      "[0.9711891  0.02881091]\n",
      "[0.03804165 0.9619584 ]\n",
      "[0.13337627 0.8666237 ]\n",
      "[0.97196746 0.02803249]\n",
      "[0.14888601 0.85111403]\n",
      "[0.51238465 0.48761535]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.97212076 0.02787927]\n",
      "[0.9705513  0.02944872]\n",
      "[0.16585408 0.8341459 ]\n",
      "[0.60676754 0.39323246]\n",
      "[0.11925583 0.88074416]\n",
      "[0.97212076 0.02787927]\n",
      "[0.90261805 0.09738193]\n",
      "[0.35645655 0.6435434 ]\n",
      "[0.9682008  0.03179925]\n",
      "[0.959667 0.040333]\n",
      "[0.03582601 0.96417403]\n",
      "[0.04828905 0.951711  ]\n",
      "[0.959667 0.040333]\n",
      "[0.9520172  0.04798277]\n",
      "[0.30008185 0.69991815]\n",
      "[0.97196746 0.02803249]\n",
      "[0.97165865 0.02834138]\n",
      "[0.04038858 0.9596114 ]\n",
      "[0.5758304 0.4241696]\n",
      "[0.0648227 0.9351773]\n",
      "[0.05764208 0.9423579 ]\n",
      "[0.02814519 0.97185487]\n",
      "[0.06873938 0.93126065]\n",
      "[0.9476931  0.05230685]\n",
      "[0.9705513  0.02944872]\n",
      "[0.60676754 0.39323246]\n",
      "[0.8632398  0.13676022]\n",
      "[0.35645655 0.6435434 ]\n",
      "[0.04550481 0.95449525]\n",
      "[0.970712   0.02928799]\n"
     ]
    }
   ],
   "source": [
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846ad44",
   "metadata": {},
   "source": [
    "Each element in the predictions list is itself a list of length 2. The sum of the two values in each list is 1. The reason for this is because the two columns contain probabilities for each possible output: experienced side effects and did not experience side effects. Each element in the predictions list is a probability distribution over all possible outputs.\n",
    "\n",
    "The first column contains the probability for each patient not experiencing side effects, which is represented by a 0. The second column contains the probability for each patient experiencing side effects, which is represented by a 1.\n",
    "\n",
    "We can also look only at the most probable prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b551292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "rounded_predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "for i in rounded_predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0f707",
   "metadata": {},
   "source": [
    "From the printed prediction results, we can observe the underlying predictions from the model, however, we cannot judge how accurate these predictions are just by looking at the predicted output.\n",
    "\n",
    "If we have corresponding labels for the test set, (for which, in this case, we do), then we can compare these true labels to the predicted labels to judge the accuracy of the model's evaluations. We'll see how to visualize this using a tool called a **confusion matrix** in the next episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1b5bc",
   "metadata": {},
   "source": [
    "## Create a Confusion Matrix for Neural Network Predictions\n",
    "\n",
    "In this episode, we'll demonstrate how to create a **confusion matrix**, which will aid us in being able to visually observe how well a neural network is predicting during inference.\n",
    "\n",
    "We'll continue working with the predictions we obtained from the **tf.keras.Sequential** model in the last episode.\n",
    "\n",
    "In the last episode, we showed how to use a trained model for inference on new data in a test set it hasn't seen before. As mentioned in that episode, we had the labels for the test set, but we didn't provide these labels to the network.\n",
    "\n",
    "Additionally, we were able to see the values that the model was predicting for each of the samples in the test set by just observing the predictions themselves.\n",
    "\n",
    "Although we were able to read the predictions from the model easily, we weren't easily able to compare the predictions to the true labels for the test data.\n",
    "\n",
    "With a **confusion matrix**, we'll be able to visually observe how well the model predicts on test data.\n",
    "\n",
    "Let's jump into the code for how this is done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b4f68",
   "metadata": {},
   "source": [
    "###  Plotting a confusion matrix\n",
    "\n",
    "First, we import all the required libraries we'll be working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59169dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded36394",
   "metadata": {},
   "source": [
    "The confusion matrix we'll be plotting comes from scikit-learn.\n",
    "\n",
    "We then create the confusion matrix and assign it to the variable **cm**.\n",
    "\n",
    "To the confusion matrix, we pass in the true labels **test_labels** as well as the network's predicted labels **rounded_predictions** for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23eda436",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a846ce",
   "metadata": {},
   "source": [
    "Below, we have a function called **plot_confusion_matrix()** that came directly from scikit-learn's website. This is code that they provide in order to plot the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab62783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9288b7e",
   "metadata": {},
   "source": [
    "Next, we define the labels for the confusion matrix. In our case, the labels are titled âno side effectsâ and âhad side effects.â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f877475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['no_side_effects','had_side_effects']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e088d5",
   "metadata": {},
   "source": [
    "Lastly, we plot the confusion matrix by using the **plot_confusion_matrix()** function we just discussed. To this function, we pass in the confusion matrix cm and the labels **cm_plot_labels**, as well as a title for the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10e217c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[191  19]\n",
      " [ 10 200]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000cfa7",
   "metadata": {},
   "source": [
    "##  Reading a Confusion Matrix \n",
    "\n",
    "Looking at the plot of the confusion matrix, we have the predicted labels on the x-axis and the true labels on the y-axis. The blue cells running from the top left to bottom right contain the number of samples that the model accurately predicted. The white cells contain the number of samples that were incorrectly predicted.\n",
    "\n",
    "There are 420 total samples in the test set. Looking at the confusion matrix, we can see that the model accurately predicted 391 out of 420 total samples. The model incorrectly predicted 29 out of the 420.\n",
    "\n",
    "For the samples the model got correct, we can see that it accurately predicted that the patients would experience no side effects 191 times. It incorrectly predicted that the patient would have no side effects 10 times when the patient did actually experience side effects.\n",
    "\n",
    "On the other side, the model accurately predicted that the patient would experience side effects 200 times that the patient did indeed experience side effects. It incorrectly predicted that the patient would have side effects 19 times when the patient actually did not experience side effects.\n",
    "\n",
    "As you can see, this is a good way we can visually interpret how well the model is doing at its predictions and understand where it may need some work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fae470",
   "metadata": {},
   "source": [
    "##  Save and load a model with TensorFlow's Keras API\n",
    "\n",
    "In this episode, we'll demonstrate how to save and load a **tf.keras.Sequential** neural network. There are a few different ways to save a Keras model. The multiple mechanisms each save the model differently, so we'll check them all out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40473f",
   "metadata": {},
   "source": [
    "###  Saving and loading the model in its entirety\n",
    "\n",
    "If we want to save a model at its current state after it was trained so that we could make use of it later, we can call the **save()** function on the model. To save(), we pass in the file path and name of the file we want to save the model to with an h5 extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c437d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "if os.path.isfile('C:/Users/sumit/OneDrive/Escritorio/Universidad/Keras with Tensorflow/models/medical_trial_model.h5') is False:\n",
    "    model.save('C:/Users/sumit/OneDrive/Escritorio/Universidad/Keras with Tensorflow/models/medical_trial_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5582b3e",
   "metadata": {},
   "source": [
    "Note, this function also allows for saving the model as a Tensorflow SavedModel as well if you'd prefer.\n",
    "\n",
    "This method of saving will save everything about the model â the architecture, the weights, the optimizer, the state of the optimizer, the learning rate, the loss, etc.\n",
    "\n",
    "Now that we have this model saved, we can load the model at a later time.\n",
    "\n",
    "To do so, we first import the **load_model()** function. Then, we can call the function to load the model by pointing to the saved model on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d1b3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('C:/Users/sumit/OneDrive/Escritorio/Universidad/Keras with Tensorflow/models/medical_trial_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ad3cc",
   "metadata": {},
   "source": [
    "We can verify that the loaded model has the same architecture and weights as the saved model by calling **summary()** and **get_weights()** on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ee18569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x16eb78d6dc0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.summary()\n",
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf956a3",
   "metadata": {},
   "source": [
    "We can also inspect attributes about the model, like the optimizer and loss by calling model.optimizer and model.loss on the loaded model and compare the results to the previously saved model.\n",
    "\n",
    "This is the most encompassing way to save and load a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38a7e4",
   "metadata": {},
   "source": [
    "###  Saving and loading only the architecture of the model\n",
    "\n",
    "There is another way we save only the **architecture of the model**. This will not save the model weights, configurations, optimizer, loss or anything else. This only saves the architecture of the model.\n",
    "\n",
    "We can do this by calling **model.to_json()**. This will save the architecture of the model as a JSON string. If we print out the string, we can see exactly what this looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e94f1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 2, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.4.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string = model.to_json()\n",
    "json_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6bd5b",
   "metadata": {},
   "source": [
    "Now that we have this saved, we can create a new model from it. First we'll import the needed **model_from_json** function, and then we can load the model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecf62d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "model_architecture = model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96dec2",
   "metadata": {},
   "source": [
    "By printing the summary of the model, we can verify that the new model has the same architecture of the model that was previously saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b79a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_architecture.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe4c19",
   "metadata": {},
   "source": [
    "Note, we can also use this same approach to saving and loading the model architecture to and from a YAML string. To do so, we use the functions **to_yaml()** and **model_from_yaml()** in the same fashion as we called the json functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95286c64",
   "metadata": {},
   "source": [
    "###  Saving and loading the weights of the model\n",
    "\n",
    "The last saving mechanism we'll discuss **only saves the weights of the model**.\n",
    "\n",
    "We can do this by calling **model.save_weights()** and passing in the path and file name to save the weights to with an h5 extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44e793d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "if os.path.isfile('C:/Users/sumit/OneDrive/Escritorio/Universidad/Keras with Tensorflow/models/medical_trial_model_weights.h5') is False:\n",
    "    model.save_weights('C:/Users/sumit/OneDrive/Escritorio/Universidad/Keras with Tensorflow/models/medical_trial_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469e5ac",
   "metadata": {},
   "source": [
    "At a later point, we could then load the saved weights in to a new model, but the new model will need to have the same architecture as the old model before the weights can be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b74e74ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.45626563, -0.01903665, -0.3260851 , -0.3672561 , -0.00110072,\n",
       "         -0.37066358, -0.3478458 ,  0.5698424 ,  0.23323748, -0.13565272,\n",
       "         -0.39531577,  0.72708803,  0.26510936, -0.3077732 ,  0.71584666,\n",
       "          0.1717615 ]], dtype=float32),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.10474867,  0.13698456,  0.        ,\n",
       "         0.        , -0.15138562, -0.09012032,  0.        , -0.15162851,\n",
       "         0.10718361], dtype=float32),\n",
       " array([[-3.03272873e-01, -2.28489533e-01,  2.29322404e-01,\n",
       "         -2.82335758e-01, -1.57067478e-01, -2.39461690e-01,\n",
       "          1.90408498e-01, -2.73221165e-01,  5.02187312e-02,\n",
       "          1.37263507e-01,  1.65481836e-01,  3.46218675e-01,\n",
       "         -2.22232640e-02, -6.20683134e-02,  3.19008619e-01,\n",
       "         -1.71003506e-01,  4.87934947e-02,  8.71600211e-02,\n",
       "         -2.89386749e-01, -1.76867217e-01, -2.72185683e-01,\n",
       "         -1.85070038e-02, -6.31268024e-02, -1.31606162e-02,\n",
       "         -1.14863992e-01, -3.23091000e-01,  1.80663168e-02,\n",
       "          3.21190089e-01,  1.01027608e-01,  1.52650565e-01,\n",
       "         -2.51663923e-02, -2.76881933e-01],\n",
       "        [ 8.63694251e-02, -2.01373085e-01, -1.18159950e-01,\n",
       "         -2.35884875e-01, -1.44729406e-01, -2.61903286e-01,\n",
       "         -1.89885542e-01,  1.71087116e-01, -2.43849188e-01,\n",
       "          2.25262314e-01, -8.82127583e-02, -8.30629170e-02,\n",
       "         -6.03924692e-02, -3.48240525e-01,  2.59583026e-01,\n",
       "         -2.21189693e-01, -1.55906081e-01,  2.64854521e-01,\n",
       "         -1.77103326e-01,  1.88137621e-01,  2.14124054e-01,\n",
       "          3.05976242e-01,  1.08506888e-01, -2.37951323e-01,\n",
       "         -1.65480405e-01, -1.15679502e-02, -2.42347836e-01,\n",
       "         -7.82988071e-02,  1.41023189e-01,  1.77300304e-01,\n",
       "         -2.52644211e-01,  2.27178067e-01],\n",
       "        [ 1.86336190e-01,  5.82328737e-02,  1.37731671e-01,\n",
       "         -2.36950159e-01, -7.88728595e-02,  3.16138119e-01,\n",
       "         -2.35131353e-01, -2.70715266e-01, -7.33762980e-04,\n",
       "          2.69829929e-02,  3.44513804e-01,  7.98040330e-02,\n",
       "          4.61804569e-02,  1.34873122e-01, -2.73361683e-01,\n",
       "          1.89555615e-01, -9.00731385e-02,  1.61381394e-01,\n",
       "         -1.78994790e-01,  2.88426071e-01, -3.04914832e-01,\n",
       "          1.60359174e-01,  3.22719604e-01, -1.63740084e-01,\n",
       "         -3.37463498e-01,  1.50199741e-01, -3.35543364e-01,\n",
       "         -1.60959065e-01,  3.42345804e-01, -2.84219086e-02,\n",
       "          3.29087466e-01, -1.80280507e-02],\n",
       "        [-1.46882951e-01,  1.84983879e-01, -3.40955108e-01,\n",
       "          1.55141443e-01, -2.95263797e-01,  4.54028547e-02,\n",
       "         -1.19430348e-01,  5.15683591e-02,  1.38860136e-01,\n",
       "         -3.17071408e-01, -7.58770406e-02,  2.10889488e-01,\n",
       "         -3.25798422e-01, -6.49595857e-02, -2.84568667e-02,\n",
       "         -4.62281704e-02, -2.98416972e-01,  1.58776373e-01,\n",
       "         -3.15432727e-01,  2.92164296e-01,  2.52897412e-01,\n",
       "          3.50773335e-05, -2.54572690e-01,  1.72629923e-01,\n",
       "         -3.02775949e-01, -1.60393119e-01,  2.27935910e-02,\n",
       "         -1.63316593e-01, -1.39057547e-01,  3.03434044e-01,\n",
       "         -9.56823528e-02,  4.34438884e-02],\n",
       "        [-2.91531354e-01, -2.54557610e-01,  2.90924579e-01,\n",
       "          3.29683095e-01,  1.13081604e-01,  3.27169687e-01,\n",
       "          2.44369894e-01, -2.31229916e-01, -1.18056446e-01,\n",
       "         -7.22011626e-02, -3.06466669e-01, -1.47480920e-01,\n",
       "          1.87920064e-01,  1.98861629e-01,  4.68646884e-02,\n",
       "          1.54111058e-01,  1.09267563e-01, -9.86096263e-02,\n",
       "         -3.75613570e-03, -1.31086707e-02,  3.19575042e-01,\n",
       "          2.44144648e-01,  2.18828052e-01,  3.18071991e-01,\n",
       "         -3.27409357e-01, -1.85321271e-02,  3.17939728e-01,\n",
       "         -1.70108467e-01,  2.31487423e-01,  1.29982471e-01,\n",
       "         -3.09659630e-01, -8.32286477e-02],\n",
       "        [ 1.38868123e-01,  1.36687964e-01, -3.00554216e-01,\n",
       "          4.76959348e-03,  4.52852249e-03, -2.20971376e-01,\n",
       "          3.06795508e-01,  1.75212711e-01,  1.59144789e-01,\n",
       "         -1.62546650e-01,  8.86503458e-02, -1.67597190e-01,\n",
       "          1.53619617e-01, -1.94855586e-01, -2.72628069e-01,\n",
       "          4.87937331e-02, -1.18919939e-01,  1.57103032e-01,\n",
       "          1.27558023e-01,  3.49236518e-01, -1.63863659e-01,\n",
       "          3.93605232e-02, -2.70522237e-01,  3.27474266e-01,\n",
       "         -1.06734365e-01, -1.92707270e-01, -2.60723114e-01,\n",
       "         -3.61427069e-02, -7.71713853e-02, -2.51563668e-01,\n",
       "         -9.23307538e-02, -3.11951578e-01],\n",
       "        [-1.88333943e-01,  2.15549082e-01,  1.47093505e-01,\n",
       "         -2.65746057e-01,  1.58442587e-01,  3.22737604e-01,\n",
       "          1.89197391e-01,  3.27665716e-01, -2.43501216e-01,\n",
       "          2.09696263e-01,  3.29216719e-02,  5.54011762e-02,\n",
       "          2.54403263e-01,  9.52980816e-02,  2.78132588e-01,\n",
       "          2.06095308e-01,  5.24884164e-02,  7.87776709e-02,\n",
       "         -3.24183613e-01,  1.32121861e-01,  6.61648214e-02,\n",
       "         -8.31586719e-02,  1.57209665e-01,  2.66422004e-01,\n",
       "          2.21469969e-01,  1.48423404e-01, -1.32704243e-01,\n",
       "          6.94385171e-03, -3.06914091e-01,  2.92842299e-01,\n",
       "         -7.74209797e-02, -1.38842836e-01],\n",
       "        [-3.92235130e-01, -7.71283656e-02,  1.13336720e-01,\n",
       "          2.45643124e-01, -4.16702271e-01,  1.90464064e-01,\n",
       "          9.04804543e-02,  1.69927880e-01,  8.56024846e-02,\n",
       "          5.04322469e-01,  2.02039078e-01,  1.75409988e-01,\n",
       "         -4.56186116e-01,  3.93809266e-02,  1.85099259e-01,\n",
       "          5.14195561e-01,  3.01147103e-01, -3.55272830e-01,\n",
       "          1.52190924e-02, -4.20583576e-01, -1.04418181e-01,\n",
       "         -1.63634613e-01, -4.11801428e-01,  4.30676609e-01,\n",
       "          3.44661266e-01,  3.74312937e-01,  3.32057863e-01,\n",
       "         -2.09426567e-01,  4.04211193e-01, -3.95058542e-01,\n",
       "         -3.31581712e-01, -5.53479306e-02],\n",
       "        [-3.55642065e-02,  2.97231942e-01, -2.64713854e-01,\n",
       "         -1.54428944e-01,  3.35098833e-01,  4.99843471e-02,\n",
       "          2.57725418e-01,  4.00694638e-01, -2.76908696e-01,\n",
       "          3.25176865e-01, -4.46094535e-02, -1.84328243e-01,\n",
       "          2.28010088e-01,  3.50839049e-01,  2.66352594e-01,\n",
       "          1.18410498e-01,  3.34378555e-02,  3.05138886e-01,\n",
       "          2.50995517e-01, -1.59872174e-01, -1.72591433e-01,\n",
       "         -7.02148080e-02,  3.55953962e-01, -2.34882399e-01,\n",
       "         -1.83959112e-01, -7.78472424e-03, -1.50116771e-01,\n",
       "         -3.43382917e-02,  2.58075029e-01,  1.86339356e-02,\n",
       "         -1.29915550e-01, -1.82411849e-01],\n",
       "        [ 2.33782738e-01,  1.94061726e-01,  1.61383480e-01,\n",
       "         -5.93520403e-02, -2.33459145e-01,  2.70001918e-01,\n",
       "         -3.29336137e-01,  3.49426895e-01, -2.11833417e-02,\n",
       "          5.88594079e-02, -1.87293887e-02, -1.05664849e-01,\n",
       "         -3.38599443e-01,  1.21265680e-01,  1.04420513e-01,\n",
       "         -9.86289382e-02,  3.10447782e-01, -2.82082617e-01,\n",
       "          1.30197406e-02,  3.34328324e-01,  3.44449610e-01,\n",
       "          6.67738318e-02, -2.15674520e-01,  1.97890133e-01,\n",
       "         -2.07974210e-01,  3.00407559e-01,  2.69728929e-01,\n",
       "          2.18944460e-01,  2.40042210e-02, -4.05748487e-02,\n",
       "          9.36182737e-02,  2.64592022e-01],\n",
       "        [-8.93530846e-02,  2.97791392e-01, -1.87130064e-01,\n",
       "         -2.30303019e-01,  8.94892216e-02,  8.29868019e-02,\n",
       "          8.18394721e-02, -9.98285115e-02, -1.66733101e-01,\n",
       "         -2.36825332e-01, -2.90735602e-01, -1.74337655e-01,\n",
       "          1.81048363e-01, -2.57783860e-01,  2.00802654e-01,\n",
       "         -1.14779517e-01,  5.19271791e-02, -2.12623939e-01,\n",
       "         -1.34554312e-01, -1.80244282e-01,  3.32698852e-01,\n",
       "          7.49252141e-02, -9.72781181e-02, -2.93575466e-01,\n",
       "          1.58266425e-02, -7.34052360e-02,  1.90881640e-01,\n",
       "         -1.75583348e-01,  2.68734723e-01,  3.20518017e-03,\n",
       "         -3.30053061e-01,  1.54002637e-01],\n",
       "        [ 1.51714861e-01, -3.91122788e-01,  4.91341725e-02,\n",
       "         -4.11524326e-01, -4.40386117e-01, -2.02849984e-01,\n",
       "         -2.61819363e-01,  5.80364347e-01, -5.48300147e-02,\n",
       "         -6.14351146e-02,  4.88566279e-01,  4.45903599e-01,\n",
       "         -1.87587485e-01, -2.92045802e-01,  3.84364754e-01,\n",
       "          4.42757696e-01, -2.14523375e-01, -4.79952157e-01,\n",
       "         -3.55477840e-01, -2.39822846e-02,  4.06787395e-02,\n",
       "         -3.19490522e-01, -4.12232697e-01,  4.35156912e-01,\n",
       "         -2.15222538e-01,  3.00012171e-01,  4.25847262e-01,\n",
       "          4.16566916e-02, -5.03636710e-02,  5.62381297e-02,\n",
       "         -1.49226397e-01, -3.33538622e-01],\n",
       "        [-2.01059669e-01, -3.69855821e-01, -1.91869080e-01,\n",
       "          1.62581444e-01, -4.06625986e-01, -3.55057925e-01,\n",
       "          3.86459120e-02,  4.28288043e-01, -1.71635106e-01,\n",
       "          3.25075209e-01,  7.04273805e-02, -1.59616880e-02,\n",
       "         -3.97457033e-01,  1.41778335e-01,  1.89090684e-01,\n",
       "          4.31911886e-01, -1.83621630e-01, -3.27351540e-01,\n",
       "         -2.31398255e-01,  9.07715783e-02, -2.49075010e-01,\n",
       "          8.47923607e-02, -2.63505816e-01,  4.77412432e-01,\n",
       "         -2.64418200e-02,  3.12258571e-01,  1.18442159e-02,\n",
       "         -3.52314889e-01,  4.66891974e-01, -7.06595331e-02,\n",
       "         -9.70438719e-02, -3.97829078e-02],\n",
       "        [-1.99886397e-01,  2.10927695e-01, -8.60016644e-02,\n",
       "          1.65302545e-01, -1.80364057e-01,  1.52448446e-01,\n",
       "          1.85554922e-02,  4.24867272e-02, -2.34303012e-01,\n",
       "          4.38976288e-02,  3.75694335e-02,  2.94124573e-01,\n",
       "          1.90767616e-01, -2.00181842e-01,  1.64971799e-01,\n",
       "          1.18635446e-01, -1.88641116e-01, -4.33863103e-02,\n",
       "         -3.35585922e-01, -2.20133319e-01, -1.80969298e-01,\n",
       "         -9.71392095e-02, -3.15926969e-02, -1.85417131e-01,\n",
       "          8.70193243e-02, -2.06210792e-01,  2.78756648e-01,\n",
       "          4.96325493e-02, -2.33182400e-01,  2.01029330e-01,\n",
       "          1.60276204e-01, -2.59908080e-01],\n",
       "        [-4.38285321e-02,  1.42330502e-03,  4.25200537e-02,\n",
       "         -2.68308014e-01,  1.70606703e-01, -3.74269187e-01,\n",
       "         -1.18451029e-01,  1.36450872e-01, -1.46981299e-01,\n",
       "          3.92237216e-01,  2.61004001e-01,  1.88834786e-01,\n",
       "          7.89420307e-02, -4.28143710e-01,  7.65847787e-02,\n",
       "          4.50404614e-01, -1.00286067e-01,  5.72752953e-02,\n",
       "         -1.46870360e-01,  1.28720552e-01, -2.83591598e-01,\n",
       "         -5.04994690e-02,  1.32042602e-01,  2.95453250e-01,\n",
       "         -1.87605485e-01, -1.16818575e-02,  2.92888939e-01,\n",
       "         -3.15556377e-01,  3.76001269e-01, -3.59214753e-01,\n",
       "         -3.36908400e-02,  6.71089161e-03],\n",
       "        [ 2.38690853e-01, -1.09781623e-01, -4.93337139e-02,\n",
       "          3.51204015e-02, -2.68552732e-02,  8.68155994e-03,\n",
       "          2.93616295e-01,  2.68170536e-01,  2.19518930e-01,\n",
       "          2.04823777e-01,  3.07994783e-01,  2.64941126e-01,\n",
       "          1.80602267e-01, -2.76848018e-01,  1.65987626e-01,\n",
       "          1.49920851e-01, -1.01134963e-01,  3.16744387e-01,\n",
       "         -2.53362596e-01,  1.83775634e-01,  2.70169199e-01,\n",
       "         -1.35306597e-01, -1.27970338e-01,  1.37004837e-01,\n",
       "         -2.31262788e-01,  4.53429297e-02, -1.45467967e-01,\n",
       "          3.09107751e-01,  2.07312882e-01,  2.13443831e-01,\n",
       "         -3.35839391e-01, -5.62900566e-02]], dtype=float32),\n",
       " array([ 0.11126692,  0.16103108,  0.09414821,  0.21464692,  0.20365389,\n",
       "         0.1936597 ,  0.10893467, -0.11740201,  0.10843661, -0.09791992,\n",
       "        -0.07279316, -0.05818616,  0.12985171,  0.21887395, -0.08879623,\n",
       "        -0.08394623, -0.00533479,  0.1647877 , -0.01109784,  0.10656041,\n",
       "         0.14942962,  0.25103772,  0.20587935, -0.06720632, -0.00051552,\n",
       "        -0.05976149, -0.04766048,  0.15747838, -0.09131765,  0.20609118,\n",
       "         0.        ,  0.2276555 ], dtype=float32),\n",
       " array([[ 0.31122085, -0.39677405],\n",
       "        [ 0.23439747, -0.28368384],\n",
       "        [ 0.2212187 ,  0.06375004],\n",
       "        [ 0.7872088 , -0.62194574],\n",
       "        [ 0.36236376, -0.7130469 ],\n",
       "        [ 0.7889526 , -0.60386825],\n",
       "        [ 0.03233676, -0.2754138 ],\n",
       "        [-0.4159317 , -0.03378041],\n",
       "        [ 0.2713621 , -0.46722442],\n",
       "        [-0.4322933 ,  0.339945  ],\n",
       "        [-0.53044105,  0.3213691 ],\n",
       "        [-0.5775658 ,  0.5740068 ],\n",
       "        [ 0.61087704, -0.6470868 ],\n",
       "        [ 0.5831427 , -0.39483255],\n",
       "        [ 0.01821346,  0.6183274 ],\n",
       "        [-0.44642046,  0.16867843],\n",
       "        [ 0.06924071,  0.18450464],\n",
       "        [ 0.5701031 , -0.79668045],\n",
       "        [-0.15057902,  0.3443022 ],\n",
       "        [ 0.40587065,  0.07638726],\n",
       "        [ 0.49809447, -0.3495147 ],\n",
       "        [ 0.5361772 , -0.52896225],\n",
       "        [ 0.2544359 , -0.73316586],\n",
       "        [-0.4397328 ,  0.12988226],\n",
       "        [-0.08242331, -0.07266199],\n",
       "        [-0.5620949 ,  0.5849949 ],\n",
       "        [-0.16579312,  0.48057234],\n",
       "        [ 0.7155788 , -0.41234717],\n",
       "        [-0.36862916,  0.50985026],\n",
       "        [ 0.43662944, -0.7561384 ],\n",
       "        [ 0.31765184,  0.06450978],\n",
       "        [ 0.28839803, -0.6555857 ]], dtype=float32),\n",
       " array([ 0.11037303, -0.11037304], dtype=float32)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.load_weights('C:/Users/sumit/OneDrive/Escritorio/Universidad/Keras with Tensorflow/models/medical_trial_model_weights.h5')\n",
    "\n",
    "model2.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea48d2b",
   "metadata": {},
   "source": [
    "We've now seen how to save only the weights of a model and deploy those weights to a new model, how to save only the architecture and then deploy that architecture to a model, and how to save everything about a model and deploy it in its entirety at a later time. Each of these saving and loading mechanisms may come in useful in differing scenarios. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
